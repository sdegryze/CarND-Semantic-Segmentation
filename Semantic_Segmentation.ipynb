{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdegryze/miniconda3/envs/semseg/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "import project_tests as tests\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdegryze/miniconda3/envs/semseg/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    \"\"\"\n",
    "    Load Pretrained VGG Model into TensorFlow.\n",
    "    :param sess: TensorFlow Session\n",
    "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
    "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    #   Use tf.saved_model.loader.load to load the model and weights\n",
    "    vgg_tag = 'vgg16'\n",
    "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    vgg_input_tensor_name = 'image_input:0'\n",
    "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
    "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
    "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
    "    \n",
    "    image_input = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "    layer3_out = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "    layer4_out = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "    layer7_out = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "    \n",
    "    return image_input, keep_prob, layer3_out, layer4_out, layer7_out\n",
    "tests.test_load_vgg(load_vgg, tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
    "    \"\"\"\n",
    "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
    "    :param vgg_layer7_out: TF Tensor for VGG Layer 3 output\n",
    "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
    "    :param vgg_layer3_out: TF Tensor for VGG Layer 7 output\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: The Tensor for the last layer of output\n",
    "    \"\"\"\n",
    "\n",
    "    # By scaling layers 3 and 4, the model learns much better (i.e. higher accuracy/IoU)\n",
    "    # See https://discussions.udacity.com/t/here-is-some-advice-and-clarifications-about-the-semantic-segmentation-project/403100\n",
    "    \n",
    "    vgg_layer3_out_scaled = tf.multiply(vgg_layer3_out, 0.0001, name='pool3_out_scaled')\n",
    "    vgg_layer4_out_scaled = tf.multiply(vgg_layer4_out, 0.01, name='pool4_out_scaled')\n",
    "\n",
    "    # First run 1x1 convolutions to collapse the number of channels/filters into just num_classes (i.e., 2)\n",
    "    # conv1_7 has 512 channels/filters\n",
    "    # Note the way we are regularizing here. We still need to add these regularization terms to the final loss\n",
    "    # Not sure if padding = 'same' is needed given that we're doing 1x1 convolutions\n",
    "    \n",
    "    conv1_7 = tf.layers.conv2d(vgg_layer7_out, num_classes, kernel_size=1, padding='same',\n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                               kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    conv1_4 = tf.layers.conv2d(vgg_layer4_out_scaled, num_classes, kernel_size=1, padding='same',\n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                               kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    conv1_3 = tf.layers.conv2d(vgg_layer3_out_scaled, num_classes, kernel_size=1, padding='same',\n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                               kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    # strides of (2, 2) is what is doing the up-sampling here\n",
    "    contrans1 = tf.layers.conv2d_transpose(conv1_7, num_classes, kernel_size=4, strides=(2, 2),\n",
    "                                           padding='same',\n",
    "                                           kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    contrans_add1 = tf.add(contrans1, conv1_4)\n",
    "    \n",
    "    contrans2 = tf.layers.conv2d_transpose(contrans_add1, num_classes, kernel_size=4, strides=(2, 2),\n",
    "                                           padding='same',\n",
    "                                           kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    contrans_add2 = tf.add(contrans2, conv1_3)\n",
    "    \n",
    "    contrans_output = tf.layers.conv2d_transpose(contrans_add2, num_classes, kernel_size=16, strides=(8, 8),\n",
    "                                                 padding='same',\n",
    "                                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    return contrans_output\n",
    "tests.test_layers(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
    "    \"\"\"\n",
    "    Build the TensorFLow loss and optimizer operations.\n",
    "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "    :param correct_label: TF Placeholder for the correct label image\n",
    "    :param learning_rate: TF Placeholder for the learning rate\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to a 2D tensor where each row represents a pixel and each column a class.\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    # why softmax, since there only 2 labels. One could just do a logistic regression.\n",
    "    # Likely, this is because a softmax is more general\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label),\n",
    "                                       name='fcn_cross_entropy_loss')\n",
    "    # add all of the regularization terms that were introduced through the kernel_regularizer arguments\n",
    "    regularization_term = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    total_loss = cross_entropy_loss + regularization_term\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('total_loss', total_loss)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    return logits, train_op, total_loss\n",
    "tests.test_optimize(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, total_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate, train_writer, merged, saver,\n",
    "             learning_rate_val=0.001,\n",
    "             keep_prob_val=0.5):\n",
    "    \"\"\"\n",
    "    Train neural network and print out the loss during training.\n",
    "    :param sess: TF Session\n",
    "    :param epochs: Number of epochs\n",
    "    :param batch_size: Batch size\n",
    "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "    :param train_op: TF Operation to train the neural network\n",
    "    :param total_loss: TF Tensor for the amount of loss\n",
    "    :param input_image: TF Placeholder for input images\n",
    "    :param correct_label: TF Placeholder for label images\n",
    "    :param keep_prob: TF Placeholder for dropout keep probability\n",
    "    :param learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_batch_nr = 0\n",
    "    for epoch in epochs:\n",
    "        batch_nr = 0\n",
    "        loss_accumul = 0\n",
    "        \n",
    "        for image, label in get_batches_fn(batch_size):\n",
    "\n",
    "            \n",
    "            _ , loss, summary = sess.run([train_op, total_loss, merged],\n",
    "                                feed_dict = {input_image: image,\n",
    "                                             correct_label: label,\n",
    "                                             learning_rate: learning_rate_val,\n",
    "                                             keep_prob: keep_prob_val})\n",
    "            loss_accumul += loss\n",
    "            print(\"{} Epoch {}, batch {}, loss {}\".format(str(datetime.datetime.now()), epoch + 1, batch_nr + 1, loss)) \n",
    "            \n",
    "            batch_nr += 1\n",
    "            overall_batch_nr += 1\n",
    "            train_writer.add_summary(summary, overall_batch_nr)\n",
    "        avg_loss = (loss_accumul / float(batch_nr))\n",
    "        print(\"Epoch {}, average loss {}\".format(epoch + 1, avg_loss))\n",
    "        model_ckpt_name = \"lr{}keep{}ep{}loss{}\".format(str(learning_rate_val).replace(\".\", \"_\"),\n",
    "                                                        str(keep_prob_val).replace(\".\", \"_\"),\n",
    "                                                        epoch + 1,\n",
    "                                                        str(avg_loss).replace(\".\", \"_\")) + \".ckpt\"\n",
    "        save_path = saver.save(sess, os.path.join(\"./models\", model_ckpt_name))\n",
    "        print(\"---------------------------------------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(start_from=None):\n",
    "    num_classes = 2\n",
    "    image_shape = (160, 576)\n",
    "    epochs = range(20)\n",
    "    batch_size = 64\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    tests.test_for_kitti_dataset(data_dir)\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
    "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
    "    #  https://www.cityscapes-dataset.com/\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Path to vgg model\n",
    "        vgg_path = os.path.join(data_dir, 'vgg')\n",
    "        # Create function to get batches\n",
    "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
    "\n",
    "        # OPTIONAL: Augment Images for better results\n",
    "        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
    "\n",
    "        correct_label = tf.placeholder(tf.float32, shape=None, name='correct_label')\n",
    "        learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate') \n",
    "        \n",
    "        image_input, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)\n",
    "        contrans_output = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
    "        logits, train_op, total_loss = optimize(contrans_output, correct_label, learning_rate, num_classes)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(\"./summaries/\", sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        if start_from is None:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            saver.restore(sess, start_from)\n",
    "            \n",
    "        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, total_loss, image_input,\n",
    "                 correct_label, keep_prob, learning_rate, train_writer, merged, saver)\n",
    "\n",
    "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n",
      "INFO:tensorflow:Restoring parameters from b'./data/vgg/variables/variables'\n",
      "2017-12-30 17:31:16.093790 Epoch 1, batch 1, loss 1.0675476789474487\n",
      "2017-12-30 17:35:22.941727 Epoch 1, batch 2, loss 1.0225756168365479\n",
      "2017-12-30 17:39:39.287631 Epoch 1, batch 3, loss 0.9825552105903625\n",
      "2017-12-30 17:43:54.721210 Epoch 1, batch 4, loss 0.9467300176620483\n",
      "2017-12-30 17:45:53.352154 Epoch 1, batch 5, loss 0.7925092577934265\n",
      "Epoch 1, average loss 0.9623835563659668\n",
      "---------------------------------------\n",
      "2017-12-30 17:50:47.919674 Epoch 2, batch 1, loss 5.346186637878418\n",
      "2017-12-30 17:54:49.108635 Epoch 2, batch 2, loss 0.7049434781074524\n",
      "2017-12-30 17:58:55.194952 Epoch 2, batch 3, loss 0.842589259147644\n",
      "2017-12-30 18:03:21.832708 Epoch 2, batch 4, loss 0.8670645356178284\n",
      "2017-12-30 18:05:33.452117 Epoch 2, batch 5, loss 0.8588822484016418\n",
      "Epoch 2, average loss 1.7239332318305969\n",
      "---------------------------------------\n",
      "2017-12-30 18:10:29.416674 Epoch 3, batch 1, loss 0.8373957276344299\n",
      "2017-12-30 18:14:44.832339 Epoch 3, batch 2, loss 0.7995320558547974\n",
      "2017-12-30 18:18:57.628642 Epoch 3, batch 3, loss 0.7979064583778381\n",
      "2017-12-30 18:23:04.429920 Epoch 3, batch 4, loss 0.7961350679397583\n",
      "2017-12-30 18:24:52.255019 Epoch 3, batch 5, loss 0.7913060188293457\n",
      "Epoch 3, average loss 0.8044550657272339\n",
      "---------------------------------------\n",
      "2017-12-30 18:29:09.946374 Epoch 4, batch 1, loss 0.7771649360656738\n",
      "2017-12-30 18:33:11.644819 Epoch 4, batch 2, loss 0.7571536898612976\n",
      "2017-12-30 18:37:05.113746 Epoch 4, batch 3, loss 0.7344285249710083\n",
      "2017-12-30 18:41:15.621373 Epoch 4, batch 4, loss 0.7063285112380981\n",
      "2017-12-30 18:43:13.551419 Epoch 4, batch 5, loss 0.6722996830940247\n",
      "Epoch 4, average loss 0.7294750690460206\n",
      "---------------------------------------\n",
      "2017-12-30 18:47:36.076070 Epoch 5, batch 1, loss 0.6123448610305786\n",
      "2017-12-30 18:51:59.384304 Epoch 5, batch 2, loss 0.6010026931762695\n",
      "2017-12-30 18:56:03.391769 Epoch 5, batch 3, loss 0.5220834016799927\n",
      "2017-12-30 19:00:00.324516 Epoch 5, batch 4, loss 0.4978345036506653\n",
      "2017-12-30 19:02:04.481405 Epoch 5, batch 5, loss 0.4504651725292206\n",
      "Epoch 5, average loss 0.5367461264133453\n",
      "---------------------------------------\n",
      "2017-12-30 19:06:38.190706 Epoch 6, batch 1, loss 0.4276905953884125\n",
      "2017-12-30 19:10:46.596435 Epoch 6, batch 2, loss 0.442840576171875\n",
      "2017-12-30 19:15:02.154793 Epoch 6, batch 3, loss 0.41626501083374023\n",
      "2017-12-30 19:19:12.922548 Epoch 6, batch 4, loss 0.4102162718772888\n",
      "2017-12-30 19:21:09.354197 Epoch 6, batch 5, loss 0.3864902853965759\n",
      "Epoch 6, average loss 0.41670054793357847\n",
      "---------------------------------------\n",
      "2017-12-30 19:25:40.302325 Epoch 7, batch 1, loss 0.3894518315792084\n",
      "2017-12-30 19:29:54.012157 Epoch 7, batch 2, loss 0.45379114151000977\n",
      "2017-12-30 19:34:00.408894 Epoch 7, batch 3, loss 0.4015210270881653\n",
      "2017-12-30 19:38:10.274381 Epoch 7, batch 4, loss 0.3899837136268616\n",
      "2017-12-30 19:40:12.931228 Epoch 7, batch 5, loss 0.41358163952827454\n",
      "Epoch 7, average loss 0.4096658706665039\n",
      "---------------------------------------\n",
      "2017-12-30 19:44:41.596906 Epoch 8, batch 1, loss 0.3909513056278229\n",
      "2017-12-30 19:48:35.719372 Epoch 8, batch 2, loss 0.3848605751991272\n",
      "2017-12-30 19:52:37.917846 Epoch 8, batch 3, loss 0.3828086256980896\n",
      "2017-12-30 19:56:40.816138 Epoch 8, batch 4, loss 0.37694841623306274\n",
      "2017-12-30 19:58:33.394155 Epoch 8, batch 5, loss 0.3709685206413269\n",
      "Epoch 8, average loss 0.38130748867988584\n",
      "---------------------------------------\n",
      "2017-12-30 20:02:58.430062 Epoch 9, batch 1, loss 0.35996729135513306\n",
      "2017-12-30 20:06:53.864704 Epoch 9, batch 2, loss 0.3579961657524109\n",
      "2017-12-30 20:10:50.144681 Epoch 9, batch 3, loss 0.3570726215839386\n",
      "2017-12-30 20:14:32.546535 Epoch 9, batch 4, loss 0.36322736740112305\n",
      "2017-12-30 20:16:14.387129 Epoch 9, batch 5, loss 0.34549617767333984\n",
      "Epoch 9, average loss 0.35675192475318906\n",
      "---------------------------------------\n",
      "2017-12-30 20:20:24.011566 Epoch 10, batch 1, loss 0.3483738899230957\n",
      "2017-12-30 20:24:12.261386 Epoch 10, batch 2, loss 0.3612348437309265\n",
      "2017-12-30 20:27:57.008393 Epoch 10, batch 3, loss 0.33812522888183594\n",
      "2017-12-30 20:31:47.463602 Epoch 10, batch 4, loss 0.34435081481933594\n",
      "2017-12-30 20:33:32.719155 Epoch 10, batch 5, loss 0.33433109521865845\n",
      "Epoch 10, average loss 0.34528317451477053\n",
      "---------------------------------------\n",
      "2017-12-30 20:37:41.371327 Epoch 11, batch 1, loss 0.34458452463150024\n",
      "2017-12-30 20:41:21.989481 Epoch 11, batch 2, loss 0.32398656010627747\n",
      "2017-12-30 20:45:17.882006 Epoch 11, batch 3, loss 0.3335234820842743\n",
      "2017-12-30 20:49:06.234225 Epoch 11, batch 4, loss 0.3232031464576721\n",
      "2017-12-30 20:50:52.853653 Epoch 11, batch 5, loss 0.30933862924575806\n",
      "Epoch 11, average loss 0.32692726850509646\n",
      "---------------------------------------\n",
      "2017-12-30 20:55:00.144510 Epoch 12, batch 1, loss 0.3194710612297058\n",
      "2017-12-30 20:58:44.725491 Epoch 12, batch 2, loss 0.31400561332702637\n",
      "2017-12-30 21:02:41.597322 Epoch 12, batch 3, loss 0.31352800130844116\n",
      "2017-12-30 21:06:35.958229 Epoch 12, batch 4, loss 0.3143993616104126\n",
      "2017-12-30 21:08:24.388500 Epoch 12, batch 5, loss 0.29751187562942505\n",
      "Epoch 12, average loss 0.3117831826210022\n",
      "---------------------------------------\n",
      "2017-12-30 21:12:43.357714 Epoch 13, batch 1, loss 0.31861716508865356\n",
      "2017-12-30 21:16:39.131977 Epoch 13, batch 2, loss 0.3356201946735382\n",
      "2017-12-30 21:20:43.468902 Epoch 13, batch 3, loss 0.3147481679916382\n",
      "2017-12-30 21:24:42.059801 Epoch 13, batch 4, loss 0.3132869601249695\n",
      "2017-12-30 21:26:33.176454 Epoch 13, batch 5, loss 0.30465275049209595\n",
      "Epoch 13, average loss 0.3173850476741791\n",
      "---------------------------------------\n",
      "2017-12-30 21:30:50.018043 Epoch 14, batch 1, loss 0.30993396043777466\n",
      "2017-12-30 21:34:46.084951 Epoch 14, batch 2, loss 0.29257869720458984\n",
      "2017-12-30 21:38:51.933728 Epoch 14, batch 3, loss 0.32807326316833496\n",
      "2017-12-30 21:42:48.919675 Epoch 14, batch 4, loss 0.28689759969711304\n",
      "2017-12-30 21:44:35.872235 Epoch 14, batch 5, loss 0.3079339265823364\n",
      "Epoch 14, average loss 0.30508348941802976\n",
      "---------------------------------------\n",
      "2017-12-30 21:48:39.617794 Epoch 15, batch 1, loss 0.29803603887557983\n",
      "2017-12-30 21:52:31.033145 Epoch 15, batch 2, loss 0.2892158627510071\n",
      "2017-12-30 21:56:31.342147 Epoch 15, batch 3, loss 0.2978297472000122\n",
      "2017-12-30 22:00:22.072418 Epoch 15, batch 4, loss 0.27966001629829407\n",
      "2017-12-30 22:02:07.042277 Epoch 15, batch 5, loss 0.2928578853607178\n",
      "Epoch 15, average loss 0.2915199100971222\n",
      "---------------------------------------\n",
      "2017-12-30 22:06:07.895474 Epoch 16, batch 1, loss 0.2854762077331543\n",
      "2017-12-30 22:10:02.758221 Epoch 16, batch 2, loss 0.28929823637008667\n",
      "2017-12-30 22:13:57.344860 Epoch 16, batch 3, loss 0.2988394796848297\n",
      "2017-12-30 22:17:52.121902 Epoch 16, batch 4, loss 0.28291282057762146\n",
      "2017-12-30 22:19:41.664525 Epoch 16, batch 5, loss 0.28072816133499146\n",
      "Epoch 16, average loss 0.28745098114013673\n",
      "---------------------------------------\n",
      "2017-12-30 22:23:51.512179 Epoch 17, batch 1, loss 0.2824537754058838\n",
      "2017-12-30 22:27:49.616278 Epoch 17, batch 2, loss 0.2755076289176941\n",
      "2017-12-30 22:31:44.050582 Epoch 17, batch 3, loss 0.27064239978790283\n",
      "2017-12-30 22:35:44.123533 Epoch 17, batch 4, loss 0.27390775084495544\n",
      "2017-12-30 22:37:47.920537 Epoch 17, batch 5, loss 0.28537461161613464\n",
      "Epoch 17, average loss 0.27757723331451417\n",
      "---------------------------------------\n",
      "2017-12-30 22:42:12.931128 Epoch 18, batch 1, loss 0.292777955532074\n",
      "2017-12-30 22:46:02.825182 Epoch 18, batch 2, loss 0.2796761989593506\n",
      "2017-12-30 22:49:48.511616 Epoch 18, batch 3, loss 0.278286874294281\n",
      "2017-12-30 22:53:36.556877 Epoch 18, batch 4, loss 0.27393582463264465\n",
      "2017-12-30 22:55:25.050143 Epoch 18, batch 5, loss 0.25579413771629333\n",
      "Epoch 18, average loss 0.2760941982269287\n",
      "---------------------------------------\n",
      "2017-12-30 22:59:24.888643 Epoch 19, batch 1, loss 0.27072691917419434\n",
      "2017-12-30 23:03:29.170415 Epoch 19, batch 2, loss 0.30246031284332275\n",
      "2017-12-30 23:07:26.213117 Epoch 19, batch 3, loss 0.27528631687164307\n",
      "2017-12-30 23:11:23.093370 Epoch 19, batch 4, loss 0.2959636449813843\n",
      "2017-12-30 23:13:16.746033 Epoch 19, batch 5, loss 0.28668659925460815\n",
      "Epoch 19, average loss 0.2862247586250305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "2017-12-30 23:17:33.962849 Epoch 20, batch 1, loss 0.27772027254104614\n",
      "2017-12-30 23:21:30.079425 Epoch 20, batch 2, loss 0.2740820646286011\n",
      "2017-12-30 23:25:27.986861 Epoch 20, batch 3, loss 0.2622261047363281\n",
      "2017-12-30 23:29:25.829257 Epoch 20, batch 4, loss 0.26541566848754883\n",
      "2017-12-30 23:31:18.190850 Epoch 20, batch 5, loss 0.2609175443649292\n",
      "Epoch 20, average loss 0.2680723309516907\n",
      "---------------------------------------\n",
      "Training Finished. Saving test images to: ./runs/1514705489.053205\n"
     ]
    }
   ],
   "source": [
    "#run(\"./models/lr0_001keep0_5ep1loss1_26573096514.ckpt\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revive():\n",
    "    num_classes = 2\n",
    "    image_shape = (160, 576)\n",
    "    epochs = range(3)\n",
    "    batch_size = 64\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    tests.test_for_kitti_dataset(data_dir)\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
    "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
    "    #  https://www.cityscapes-dataset.com/\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Path to vgg model\n",
    "        model_path = os.path.join(\"./models\", 'lr0_001keep0_5ep1loss1_26573096514.ckpt')\n",
    "        # Create function to get batches\n",
    "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
    "\n",
    "        \n",
    "        imported_meta = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "        imported_meta.restore(sess, model_path)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        fcn_cross_entropy_loss_name = 'fcn_cross_entropy_loss:0'\n",
    "        cross_entropy_loss = graph.get_tensor_by_name(fcn_cross_entropy_loss_name)\n",
    "        vgg_input_tensor_name = 'image_input:0'\n",
    "        vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "\n",
    "        input_image = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "        keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "        correct_label = graph.get_tensor_by_name('correct_label:0')\n",
    "        \n",
    "        batch_nr = 0\n",
    "        batch_size = 64\n",
    "        loss_accumul = 0\n",
    "        for image, label in get_batches_fn(batch_size):\n",
    "            loss = sess.run(cross_entropy_loss,\n",
    "                                feed_dict = {input_image: image,\n",
    "                                             correct_label: label,\n",
    "                                             keep_prob: 1})\n",
    "            loss_accumul += loss\n",
    "            print(\"Batch {}, loss {}\".format(batch_nr + 1, loss)) \n",
    "            batch_nr += 1\n",
    "        avg_loss = (loss_accumul / float(batch_nr))\n",
    "        print(\"Average loss  = {}\".format(avg_loss))\n",
    "revive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
