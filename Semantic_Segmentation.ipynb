{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdegryze/miniconda3/envs/semseg/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "import project_tests as tests\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdegryze/miniconda3/envs/semseg/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    \"\"\"\n",
    "    Load Pretrained VGG Model into TensorFlow.\n",
    "    :param sess: TensorFlow Session\n",
    "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
    "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    #   Use tf.saved_model.loader.load to load the model and weights\n",
    "    vgg_tag = 'vgg16'\n",
    "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    vgg_input_tensor_name = 'image_input:0'\n",
    "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
    "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
    "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
    "    \n",
    "    image_input = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "    layer3_out = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "    layer4_out = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "    layer7_out = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "    \n",
    "    return image_input, keep_prob, layer3_out, layer4_out, layer7_out\n",
    "tests.test_load_vgg(load_vgg, tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
    "    \"\"\"\n",
    "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
    "    :param vgg_layer7_out: TF Tensor for VGG Layer 3 output\n",
    "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
    "    :param vgg_layer3_out: TF Tensor for VGG Layer 7 output\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: The Tensor for the last layer of output\n",
    "    \"\"\"\n",
    "\n",
    "    # By scaling layers 3 and 4, the model learns much better (i.e. higher accuracy/IoU)\n",
    "    # See https://discussions.udacity.com/t/here-is-some-advice-and-clarifications-about-the-semantic-segmentation-project/403100\n",
    "    \n",
    "    vgg_layer3_out_scaled = tf.multiply(vgg_layer3_out, 0.0001, name='pool3_out_scaled')\n",
    "    vgg_layer4_out_scaled = tf.multiply(vgg_layer4_out, 0.01, name='pool4_out_scaled')\n",
    "\n",
    "    # First run 1x1 convolutions to collapse the number of channels/filters into just num_classes (i.e., 2)\n",
    "    # conv1_7 has 512 channels/filters\n",
    "    # Note the way we are regularizing here. We still need to add these regularization terms to the final loss\n",
    "    # Not sure if padding = 'same' is needed given that we're doing 1x1 convolutions\n",
    "    \n",
    "    conv1_7 = tf.layers.conv2d(vgg_layer7_out, num_classes, kernel_size=1, padding='same',\n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                               kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    conv1_4 = tf.layers.conv2d(vgg_layer4_out_scaled, num_classes, kernel_size=1, padding='same',\n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                               kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    conv1_3 = tf.layers.conv2d(vgg_layer3_out_scaled, num_classes, kernel_size=1, padding='same',\n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                               kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    # strides of (2, 2) is what is doing the up-sampling here\n",
    "    contrans1 = tf.layers.conv2d_transpose(conv1_7, num_classes, kernel_size=4, strides=(2, 2),\n",
    "                                           padding='same',\n",
    "                                           kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    contrans_add1 = tf.add(contrans1, conv1_4)\n",
    "    \n",
    "    contrans2 = tf.layers.conv2d_transpose(contrans_add1, num_classes, kernel_size=4, strides=(2, 2),\n",
    "                                           padding='same',\n",
    "                                           kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    contrans_add2 = tf.add(contrans2, conv1_3)\n",
    "    \n",
    "    contrans_output = tf.layers.conv2d_transpose(contrans_add2, num_classes, kernel_size=16, strides=(8, 8),\n",
    "                                                 padding='same',\n",
    "                                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    return contrans_output\n",
    "tests.test_layers(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
    "    \"\"\"\n",
    "    Build the TensorFLow loss and optimizer operations.\n",
    "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "    :param correct_label: TF Placeholder for the correct label image\n",
    "    :param learning_rate: TF Placeholder for the learning rate\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to a 2D tensor where each row represents a pixel and each column a class.\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    # why softmax, since there only 2 labels. One could just do a logistic regression.\n",
    "    # Likely, this is because a softmax is more general\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label),\n",
    "                                       name='fcn_cross_entropy_loss')\n",
    "    # add all of the regularization terms that were introduced through the kernel_regularizer arguments\n",
    "    regularization_term = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    total_loss = cross_entropy_loss + regularization_term\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('total_loss', total_loss)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    return logits, train_op, total_loss\n",
    "tests.test_optimize(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, total_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate, train_writer, merged, saver,\n",
    "             learning_rate_val=0.001,\n",
    "             keep_prob_val=0.5):\n",
    "    \"\"\"\n",
    "    Train neural network and print out the loss during training.\n",
    "    :param sess: TF Session\n",
    "    :param epochs: Number of epochs\n",
    "    :param batch_size: Batch size\n",
    "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "    :param train_op: TF Operation to train the neural network\n",
    "    :param total_loss: TF Tensor for the amount of loss\n",
    "    :param input_image: TF Placeholder for input images\n",
    "    :param correct_label: TF Placeholder for label images\n",
    "    :param keep_prob: TF Placeholder for dropout keep probability\n",
    "    :param learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_batch_nr = 0\n",
    "    for epoch in epochs:\n",
    "        batch_nr = 0\n",
    "        loss_accumul = 0\n",
    "        \n",
    "        for image, label in get_batches_fn(batch_size):\n",
    "\n",
    "            \n",
    "            _ , loss, summary = sess.run([train_op, total_loss, merged],\n",
    "                                feed_dict = {input_image: image,\n",
    "                                             correct_label: label,\n",
    "                                             learning_rate: learning_rate_val,\n",
    "                                             keep_prob: keep_prob_val})\n",
    "            loss_accumul += loss\n",
    "            print(\"{} Epoch {}, batch {}, loss {}\".format(str(datetime.datetime.now()), epoch + 1, batch_nr + 1, loss)) \n",
    "            \n",
    "            batch_nr += 1\n",
    "            overall_batch_nr += 1\n",
    "            train_writer.add_summary(summary, overall_batch_nr)\n",
    "        avg_loss = (loss_accumul / float(batch_nr))\n",
    "        print(\"Epoch {}, average loss {}\".format(epoch + 1, avg_loss))\n",
    "        model_ckpt_name = \"lr{}keep{}ep{}loss{}\".format(str(learning_rate_val).replace(\".\", \"_\"),\n",
    "                                                        str(keep_prob_val).replace(\".\", \"_\"),\n",
    "                                                        epoch + 1,\n",
    "                                                        str(avg_loss).replace(\".\", \"_\")) + \".ckpt\"\n",
    "        save_path = saver.save(sess, os.path.join(\"./models\", model_ckpt_name))\n",
    "        print(\"---------------------------------------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(start_from=None):\n",
    "    num_classes = 2\n",
    "    image_shape = (160, 576)\n",
    "    epochs = range(20)\n",
    "    batch_size = 8\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    tests.test_for_kitti_dataset(data_dir)\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
    "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
    "    #  https://www.cityscapes-dataset.com/\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Path to vgg model\n",
    "        vgg_path = os.path.join(data_dir, 'vgg')\n",
    "        # Create function to get batches\n",
    "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
    "\n",
    "        # OPTIONAL: Augment Images for better results\n",
    "        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
    "\n",
    "        correct_label = tf.placeholder(tf.float32, shape=None, name='correct_label')\n",
    "        learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate') \n",
    "        \n",
    "        image_input, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)\n",
    "        contrans_output = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
    "        logits, train_op, total_loss = optimize(contrans_output, correct_label, learning_rate, num_classes)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(\"./summaries/\", sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        if start_from is None:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            saver.restore(sess, start_from)\n",
    "            \n",
    "        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, total_loss, image_input,\n",
    "                 correct_label, keep_prob, learning_rate, train_writer, merged, saver)\n",
    "\n",
    "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n",
      "INFO:tensorflow:Restoring parameters from b'./data/vgg/variables/variables'\n",
      "INFO:tensorflow:Restoring parameters from ./models/lr0_001keep0_5ep20loss0_268072330952.ckpt\n",
      "2018-01-08 21:38:15.035415 Epoch 1, batch 1, loss 0.23808833956718445\n",
      "2018-01-08 21:38:40.491349 Epoch 1, batch 2, loss 0.27822378277778625\n",
      "2018-01-08 21:39:07.640138 Epoch 1, batch 3, loss 0.2699750065803528\n",
      "2018-01-08 21:39:35.203568 Epoch 1, batch 4, loss 0.2716173529624939\n",
      "2018-01-08 21:40:04.411240 Epoch 1, batch 5, loss 0.3064248561859131\n",
      "2018-01-08 21:40:32.943649 Epoch 1, batch 6, loss 0.30008578300476074\n",
      "2018-01-08 21:40:59.362683 Epoch 1, batch 7, loss 0.2723190188407898\n",
      "2018-01-08 21:41:25.751734 Epoch 1, batch 8, loss 0.2788800001144409\n",
      "2018-01-08 21:41:52.485755 Epoch 1, batch 9, loss 0.29633527994155884\n",
      "2018-01-08 21:42:19.213192 Epoch 1, batch 10, loss 0.29033660888671875\n",
      "2018-01-08 21:42:46.131780 Epoch 1, batch 11, loss 0.31081679463386536\n",
      "2018-01-08 21:43:13.234533 Epoch 1, batch 12, loss 0.2855260372161865\n",
      "2018-01-08 21:43:40.432627 Epoch 1, batch 13, loss 0.2651524841785431\n",
      "2018-01-08 21:44:07.799285 Epoch 1, batch 14, loss 0.26988738775253296\n",
      "2018-01-08 21:44:34.589159 Epoch 1, batch 15, loss 0.2803853750228882\n",
      "2018-01-08 21:45:01.356206 Epoch 1, batch 16, loss 0.2869057059288025\n",
      "2018-01-08 21:45:28.933081 Epoch 1, batch 17, loss 0.26164692640304565\n",
      "2018-01-08 21:45:55.218560 Epoch 1, batch 18, loss 0.2904232144355774\n",
      "2018-01-08 21:46:23.979168 Epoch 1, batch 19, loss 0.349334716796875\n",
      "2018-01-08 21:46:52.083746 Epoch 1, batch 20, loss 0.26125115156173706\n",
      "2018-01-08 21:47:20.665736 Epoch 1, batch 21, loss 0.3432331085205078\n",
      "2018-01-08 21:47:49.411066 Epoch 1, batch 22, loss 0.3277623951435089\n",
      "2018-01-08 21:48:17.447100 Epoch 1, batch 23, loss 0.3160342574119568\n",
      "2018-01-08 21:48:45.117304 Epoch 1, batch 24, loss 0.3086516857147217\n",
      "2018-01-08 21:49:11.781257 Epoch 1, batch 25, loss 0.31209832429885864\n",
      "2018-01-08 21:49:37.039484 Epoch 1, batch 26, loss 0.34034857153892517\n",
      "2018-01-08 21:50:02.651685 Epoch 1, batch 27, loss 0.3044432997703552\n",
      "2018-01-08 21:50:27.916905 Epoch 1, batch 28, loss 0.2934589982032776\n",
      "2018-01-08 21:50:53.444139 Epoch 1, batch 29, loss 0.2788403034210205\n",
      "2018-01-08 21:51:18.987627 Epoch 1, batch 30, loss 0.271780788898468\n",
      "2018-01-08 21:51:44.877069 Epoch 1, batch 31, loss 0.31864094734191895\n",
      "2018-01-08 21:52:10.744844 Epoch 1, batch 32, loss 0.3033081293106079\n",
      "2018-01-08 21:52:36.379614 Epoch 1, batch 33, loss 0.29193544387817383\n",
      "2018-01-08 21:53:02.196363 Epoch 1, batch 34, loss 0.3100716471672058\n",
      "2018-01-08 21:53:27.364792 Epoch 1, batch 35, loss 0.29716283082962036\n",
      "2018-01-08 21:53:52.559111 Epoch 1, batch 36, loss 0.2398829460144043\n",
      "2018-01-08 21:53:56.203694 Epoch 1, batch 37, loss 0.2600034177303314\n",
      "Epoch 1, average loss 0.2913857545401599\n",
      "---------------------------------------\n",
      "2018-01-08 21:54:30.739912 Epoch 2, batch 1, loss 0.30225661396980286\n",
      "2018-01-08 21:54:55.857901 Epoch 2, batch 2, loss 0.2863357663154602\n",
      "2018-01-08 21:55:20.762069 Epoch 2, batch 3, loss 0.2871924638748169\n",
      "2018-01-08 21:55:45.848215 Epoch 2, batch 4, loss 0.3023023009300232\n",
      "2018-01-08 21:56:10.759600 Epoch 2, batch 5, loss 0.28560441732406616\n",
      "2018-01-08 21:56:35.975405 Epoch 2, batch 6, loss 0.38184666633605957\n",
      "2018-01-08 21:57:01.004573 Epoch 2, batch 7, loss 0.30247238278388977\n",
      "2018-01-08 21:57:25.946756 Epoch 2, batch 8, loss 0.302297979593277\n",
      "2018-01-08 21:57:51.421027 Epoch 2, batch 9, loss 0.3553928732872009\n",
      "2018-01-08 21:58:16.974841 Epoch 2, batch 10, loss 0.25832313299179077\n",
      "2018-01-08 21:58:42.605441 Epoch 2, batch 11, loss 0.3015134036540985\n",
      "2018-01-08 21:59:08.326965 Epoch 2, batch 12, loss 0.2779529094696045\n",
      "2018-01-08 21:59:33.479182 Epoch 2, batch 13, loss 0.28062692284584045\n",
      "2018-01-08 21:59:58.662872 Epoch 2, batch 14, loss 0.31076252460479736\n",
      "2018-01-08 22:00:23.776900 Epoch 2, batch 15, loss 0.27604061365127563\n",
      "2018-01-08 22:00:49.193115 Epoch 2, batch 16, loss 0.30635547637939453\n",
      "2018-01-08 22:01:14.318393 Epoch 2, batch 17, loss 0.29398834705352783\n",
      "2018-01-08 22:01:39.163112 Epoch 2, batch 18, loss 0.33285754919052124\n",
      "2018-01-08 22:02:04.116241 Epoch 2, batch 19, loss 0.3300527334213257\n",
      "2018-01-08 22:02:29.198191 Epoch 2, batch 20, loss 0.2821628749370575\n",
      "2018-01-08 22:02:54.177636 Epoch 2, batch 21, loss 0.28847354650497437\n",
      "2018-01-08 22:03:19.192898 Epoch 2, batch 22, loss 0.30736666917800903\n",
      "2018-01-08 22:03:44.592389 Epoch 2, batch 23, loss 0.2799600064754486\n",
      "2018-01-08 22:04:10.107443 Epoch 2, batch 24, loss 0.2230590283870697\n",
      "2018-01-08 22:04:35.075115 Epoch 2, batch 25, loss 0.3465006351470947\n",
      "2018-01-08 22:04:59.920120 Epoch 2, batch 26, loss 0.29990461468696594\n",
      "2018-01-08 22:05:25.221470 Epoch 2, batch 27, loss 0.28365516662597656\n",
      "2018-01-08 22:05:50.052240 Epoch 2, batch 28, loss 0.2917749881744385\n",
      "2018-01-08 22:06:14.821624 Epoch 2, batch 29, loss 0.32970738410949707\n",
      "2018-01-08 22:06:40.094978 Epoch 2, batch 30, loss 0.28576555848121643\n",
      "2018-01-08 22:07:04.947484 Epoch 2, batch 31, loss 0.25392743945121765\n",
      "2018-01-08 22:07:29.745012 Epoch 2, batch 32, loss 0.2816120386123657\n",
      "2018-01-08 22:07:54.776923 Epoch 2, batch 33, loss 0.2658519744873047\n",
      "2018-01-08 22:08:19.881510 Epoch 2, batch 34, loss 0.3198509216308594\n",
      "2018-01-08 22:08:44.789500 Epoch 2, batch 35, loss 0.277221143245697\n",
      "2018-01-08 22:09:09.638125 Epoch 2, batch 36, loss 0.31457194685935974\n",
      "2018-01-08 22:09:13.227124 Epoch 2, batch 37, loss 0.2821837067604065\n",
      "Epoch 2, average loss 0.29696553301166845\n",
      "---------------------------------------\n",
      "2018-01-08 22:09:47.304184 Epoch 3, batch 1, loss 0.3119574785232544\n",
      "2018-01-08 22:10:12.039947 Epoch 3, batch 2, loss 0.3522336483001709\n",
      "2018-01-08 22:10:37.191941 Epoch 3, batch 3, loss 0.29260849952697754\n",
      "2018-01-08 22:11:01.900343 Epoch 3, batch 4, loss 0.34468942880630493\n",
      "2018-01-08 22:11:26.634682 Epoch 3, batch 5, loss 0.34409868717193604\n",
      "2018-01-08 22:11:51.252668 Epoch 3, batch 6, loss 0.31780609488487244\n",
      "2018-01-08 22:12:15.962043 Epoch 3, batch 7, loss 0.3043269217014313\n",
      "2018-01-08 22:12:40.585379 Epoch 3, batch 8, loss 0.25812777876853943\n",
      "2018-01-08 22:13:05.440912 Epoch 3, batch 9, loss 0.23365500569343567\n",
      "2018-01-08 22:13:30.186609 Epoch 3, batch 10, loss 0.28639400005340576\n",
      "2018-01-08 22:13:55.002897 Epoch 3, batch 11, loss 0.3331196904182434\n",
      "2018-01-08 22:14:19.954778 Epoch 3, batch 12, loss 0.2681625485420227\n",
      "2018-01-08 22:14:44.652869 Epoch 3, batch 13, loss 0.27825498580932617\n",
      "2018-01-08 22:15:09.356387 Epoch 3, batch 14, loss 0.27382200956344604\n",
      "2018-01-08 22:15:34.143977 Epoch 3, batch 15, loss 0.2731360197067261\n",
      "2018-01-08 22:15:59.083446 Epoch 3, batch 16, loss 0.280426949262619\n",
      "2018-01-08 22:16:23.301545 Epoch 3, batch 17, loss 0.26768267154693604\n",
      "2018-01-08 22:16:48.128629 Epoch 3, batch 18, loss 0.27677807211875916\n",
      "2018-01-08 22:17:12.723787 Epoch 3, batch 19, loss 0.2631613314151764\n",
      "2018-01-08 22:17:37.360762 Epoch 3, batch 20, loss 0.2659454345703125\n",
      "2018-01-08 22:18:02.697922 Epoch 3, batch 21, loss 0.26203083992004395\n",
      "2018-01-08 22:18:27.625670 Epoch 3, batch 22, loss 0.2572944462299347\n",
      "2018-01-08 22:18:52.295435 Epoch 3, batch 23, loss 0.24160844087600708\n",
      "2018-01-08 22:19:17.538211 Epoch 3, batch 24, loss 0.23197993636131287\n",
      "2018-01-08 22:19:43.117921 Epoch 3, batch 25, loss 0.2786147892475128\n",
      "2018-01-08 22:20:07.849198 Epoch 3, batch 26, loss 0.2688191533088684\n",
      "2018-01-08 22:20:32.078579 Epoch 3, batch 27, loss 0.2978610694408417\n",
      "2018-01-08 22:20:56.420867 Epoch 3, batch 28, loss 0.2879815995693207\n",
      "2018-01-08 22:21:20.649886 Epoch 3, batch 29, loss 0.2631784677505493\n",
      "2018-01-08 22:21:44.894890 Epoch 3, batch 30, loss 0.2756306529045105\n",
      "2018-01-08 22:22:09.226631 Epoch 3, batch 31, loss 0.23483189940452576\n",
      "2018-01-08 22:22:33.659137 Epoch 3, batch 32, loss 0.2612515985965729\n",
      "2018-01-08 22:22:58.078736 Epoch 3, batch 33, loss 0.27412816882133484\n",
      "2018-01-08 22:23:22.756090 Epoch 3, batch 34, loss 0.2802077829837799\n",
      "2018-01-08 22:23:49.083663 Epoch 3, batch 35, loss 0.2780081629753113\n",
      "2018-01-08 22:24:15.805662 Epoch 3, batch 36, loss 0.2958228290081024\n",
      "2018-01-08 22:24:19.467916 Epoch 3, batch 37, loss 0.24088212847709656\n",
      "Epoch 3, average loss 0.279905924925933\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08 22:24:52.704858 Epoch 4, batch 1, loss 0.2421664297580719\n",
      "2018-01-08 22:25:18.252090 Epoch 4, batch 2, loss 0.3192604184150696\n",
      "2018-01-08 22:25:43.933100 Epoch 4, batch 3, loss 0.2365300953388214\n",
      "2018-01-08 22:26:09.309423 Epoch 4, batch 4, loss 0.2597091794013977\n",
      "2018-01-08 22:26:34.689877 Epoch 4, batch 5, loss 0.2633725702762604\n",
      "2018-01-08 22:26:59.448842 Epoch 4, batch 6, loss 0.27253687381744385\n",
      "2018-01-08 22:27:23.545012 Epoch 4, batch 7, loss 0.25634390115737915\n",
      "2018-01-08 22:27:47.850638 Epoch 4, batch 8, loss 0.25695252418518066\n",
      "2018-01-08 22:28:12.173837 Epoch 4, batch 9, loss 0.26156240701675415\n",
      "2018-01-08 22:28:36.735134 Epoch 4, batch 10, loss 0.25735989212989807\n",
      "2018-01-08 22:29:01.108323 Epoch 4, batch 11, loss 0.2503288984298706\n",
      "2018-01-08 22:29:25.200768 Epoch 4, batch 12, loss 0.2527956962585449\n",
      "2018-01-08 22:29:49.401521 Epoch 4, batch 13, loss 0.2740842401981354\n",
      "2018-01-08 22:30:13.838264 Epoch 4, batch 14, loss 0.24520167708396912\n",
      "2018-01-08 22:30:38.068744 Epoch 4, batch 15, loss 0.23577198386192322\n",
      "2018-01-08 22:31:02.354529 Epoch 4, batch 16, loss 0.25896820425987244\n",
      "2018-01-08 22:31:26.521520 Epoch 4, batch 17, loss 0.23931840062141418\n",
      "2018-01-08 22:31:50.977497 Epoch 4, batch 18, loss 0.25428009033203125\n",
      "2018-01-08 22:32:15.336703 Epoch 4, batch 19, loss 0.2551608085632324\n",
      "2018-01-08 22:32:39.664163 Epoch 4, batch 20, loss 0.2851288914680481\n",
      "2018-01-08 22:33:03.908896 Epoch 4, batch 21, loss 0.2679266333580017\n",
      "2018-01-08 22:33:28.368944 Epoch 4, batch 22, loss 0.24955296516418457\n",
      "2018-01-08 22:33:52.852586 Epoch 4, batch 23, loss 0.31049060821533203\n",
      "2018-01-08 22:34:18.435313 Epoch 4, batch 24, loss 0.2375674843788147\n",
      "2018-01-08 22:34:42.806709 Epoch 4, batch 25, loss 0.2507411539554596\n",
      "2018-01-08 22:35:07.207325 Epoch 4, batch 26, loss 0.238051176071167\n",
      "2018-01-08 22:35:31.458211 Epoch 4, batch 27, loss 0.23903758823871613\n",
      "2018-01-08 22:35:55.736475 Epoch 4, batch 28, loss 0.232340008020401\n",
      "2018-01-08 22:36:20.008594 Epoch 4, batch 29, loss 0.22948181629180908\n",
      "2018-01-08 22:36:44.550333 Epoch 4, batch 30, loss 0.23457300662994385\n",
      "2018-01-08 22:37:08.964107 Epoch 4, batch 31, loss 0.2445918619632721\n",
      "2018-01-08 22:37:33.187159 Epoch 4, batch 32, loss 0.249919593334198\n",
      "2018-01-08 22:37:57.752711 Epoch 4, batch 33, loss 0.2560318112373352\n",
      "2018-01-08 22:38:22.168139 Epoch 4, batch 34, loss 0.24012666940689087\n",
      "2018-01-08 22:38:46.390754 Epoch 4, batch 35, loss 0.22992807626724243\n",
      "2018-01-08 22:39:10.691875 Epoch 4, batch 36, loss 0.25393328070640564\n",
      "2018-01-08 22:39:14.211298 Epoch 4, batch 37, loss 0.21922506392002106\n",
      "Epoch 4, average loss 0.2529824859387166\n",
      "---------------------------------------\n",
      "2018-01-08 22:39:46.840873 Epoch 5, batch 1, loss 0.2435373067855835\n",
      "2018-01-08 22:40:11.011692 Epoch 5, batch 2, loss 0.24668481945991516\n",
      "2018-01-08 22:40:35.158147 Epoch 5, batch 3, loss 0.24620801210403442\n",
      "2018-01-08 22:40:59.442448 Epoch 5, batch 4, loss 0.2414112687110901\n",
      "2018-01-08 22:41:23.751455 Epoch 5, batch 5, loss 0.21743187308311462\n",
      "2018-01-08 22:41:47.826961 Epoch 5, batch 6, loss 0.2235833704471588\n",
      "2018-01-08 22:42:11.964570 Epoch 5, batch 7, loss 0.24639800190925598\n",
      "2018-01-08 22:42:36.862677 Epoch 5, batch 8, loss 0.22415442764759064\n",
      "2018-01-08 22:43:01.112074 Epoch 5, batch 9, loss 0.24876418709754944\n",
      "2018-01-08 22:43:25.501056 Epoch 5, batch 10, loss 0.24431777000427246\n",
      "2018-01-08 22:43:49.728271 Epoch 5, batch 11, loss 0.2567930221557617\n",
      "2018-01-08 22:44:14.021082 Epoch 5, batch 12, loss 0.24034009873867035\n",
      "2018-01-08 22:44:38.176483 Epoch 5, batch 13, loss 0.24422705173492432\n",
      "2018-01-08 22:45:02.330133 Epoch 5, batch 14, loss 0.23849692940711975\n",
      "2018-01-08 22:45:26.536479 Epoch 5, batch 15, loss 0.2860894799232483\n",
      "2018-01-08 22:45:51.115738 Epoch 5, batch 16, loss 0.23288585245609283\n",
      "2018-01-08 22:46:15.509204 Epoch 5, batch 17, loss 0.2504158318042755\n",
      "2018-01-08 22:46:39.802935 Epoch 5, batch 18, loss 0.22948329150676727\n",
      "2018-01-08 22:47:04.192616 Epoch 5, batch 19, loss 0.2767396569252014\n",
      "2018-01-08 22:47:28.770017 Epoch 5, batch 20, loss 0.22413015365600586\n",
      "2018-01-08 22:47:53.328370 Epoch 5, batch 21, loss 0.22425779700279236\n",
      "2018-01-08 22:48:17.494695 Epoch 5, batch 22, loss 0.29142066836357117\n",
      "2018-01-08 22:48:41.809255 Epoch 5, batch 23, loss 0.21710160374641418\n",
      "2018-01-08 22:49:06.062861 Epoch 5, batch 24, loss 0.23544058203697205\n",
      "2018-01-08 22:49:30.437014 Epoch 5, batch 25, loss 0.24204054474830627\n",
      "2018-01-08 22:49:54.715885 Epoch 5, batch 26, loss 0.22250616550445557\n",
      "2018-01-08 22:50:19.314002 Epoch 5, batch 27, loss 0.22149842977523804\n",
      "2018-01-08 22:50:43.643224 Epoch 5, batch 28, loss 0.26692846417427063\n",
      "2018-01-08 22:51:08.070951 Epoch 5, batch 29, loss 0.23205657303333282\n",
      "2018-01-08 22:51:32.569946 Epoch 5, batch 30, loss 0.25681012868881226\n",
      "2018-01-08 22:51:56.886794 Epoch 5, batch 31, loss 0.22175712883472443\n",
      "2018-01-08 22:52:21.139525 Epoch 5, batch 32, loss 0.27537304162979126\n",
      "2018-01-08 22:52:45.519860 Epoch 5, batch 33, loss 0.2325548529624939\n",
      "2018-01-08 22:53:10.128202 Epoch 5, batch 34, loss 0.2502146065235138\n",
      "2018-01-08 22:53:34.514967 Epoch 5, batch 35, loss 0.23542353510856628\n",
      "2018-01-08 22:53:58.564386 Epoch 5, batch 36, loss 0.22530195116996765\n",
      "2018-01-08 22:54:02.115649 Epoch 5, batch 37, loss 0.2254466712474823\n",
      "Epoch 5, average loss 0.24157365270563075\n",
      "---------------------------------------\n",
      "2018-01-08 22:54:34.230325 Epoch 6, batch 1, loss 0.29280000925064087\n",
      "2018-01-08 22:54:58.537235 Epoch 6, batch 2, loss 0.22454142570495605\n",
      "2018-01-08 22:55:22.697421 Epoch 6, batch 3, loss 0.265983521938324\n",
      "2018-01-08 22:55:47.138322 Epoch 6, batch 4, loss 0.2862909436225891\n",
      "2018-01-08 22:56:11.442525 Epoch 6, batch 5, loss 0.2737797498703003\n",
      "2018-01-08 22:56:35.764792 Epoch 6, batch 6, loss 0.24713698029518127\n",
      "2018-01-08 22:57:00.081391 Epoch 6, batch 7, loss 0.2714189887046814\n",
      "2018-01-08 22:57:24.384740 Epoch 6, batch 8, loss 0.24201902747154236\n",
      "2018-01-08 22:57:48.591355 Epoch 6, batch 9, loss 0.27197933197021484\n",
      "2018-01-08 22:58:13.189732 Epoch 6, batch 10, loss 0.3187779188156128\n",
      "2018-01-08 22:58:37.571581 Epoch 6, batch 11, loss 0.23451191186904907\n",
      "2018-01-08 22:59:01.987847 Epoch 6, batch 12, loss 0.23256278038024902\n",
      "2018-01-08 22:59:26.183508 Epoch 6, batch 13, loss 0.23504824936389923\n",
      "2018-01-08 22:59:50.694902 Epoch 6, batch 14, loss 0.3079974055290222\n",
      "2018-01-08 23:00:15.180902 Epoch 6, batch 15, loss 0.22625726461410522\n",
      "2018-01-08 23:00:39.648340 Epoch 6, batch 16, loss 0.24177615344524384\n",
      "2018-01-08 23:01:03.977361 Epoch 6, batch 17, loss 0.22706961631774902\n",
      "2018-01-08 23:01:28.185405 Epoch 6, batch 18, loss 0.24465733766555786\n",
      "2018-01-08 23:01:52.821540 Epoch 6, batch 19, loss 0.23243896663188934\n",
      "2018-01-08 23:02:17.122008 Epoch 6, batch 20, loss 0.2740296721458435\n",
      "2018-01-08 23:02:41.468974 Epoch 6, batch 21, loss 0.2808707654476166\n",
      "2018-01-08 23:03:06.097217 Epoch 6, batch 22, loss 0.2670697271823883\n",
      "2018-01-08 23:03:30.510434 Epoch 6, batch 23, loss 0.2180393934249878\n",
      "2018-01-08 23:03:54.891298 Epoch 6, batch 24, loss 0.28582513332366943\n",
      "2018-01-08 23:04:19.346165 Epoch 6, batch 25, loss 0.21997487545013428\n",
      "2018-01-08 23:04:43.912800 Epoch 6, batch 26, loss 0.22202342748641968\n",
      "2018-01-08 23:05:08.204173 Epoch 6, batch 27, loss 0.2250847965478897\n",
      "2018-01-08 23:05:32.642251 Epoch 6, batch 28, loss 0.23117803037166595\n",
      "2018-01-08 23:05:56.996541 Epoch 6, batch 29, loss 0.24282965064048767\n",
      "2018-01-08 23:06:21.664226 Epoch 6, batch 30, loss 0.24960359930992126\n",
      "2018-01-08 23:06:46.103263 Epoch 6, batch 31, loss 0.22063028812408447\n",
      "2018-01-08 23:07:10.526536 Epoch 6, batch 32, loss 0.2331118881702423\n",
      "2018-01-08 23:07:34.982477 Epoch 6, batch 33, loss 0.22123265266418457\n",
      "2018-01-08 23:07:59.495001 Epoch 6, batch 34, loss 0.20462681353092194\n",
      "2018-01-08 23:08:23.740531 Epoch 6, batch 35, loss 0.20346422493457794\n",
      "2018-01-08 23:08:48.042539 Epoch 6, batch 36, loss 0.21107204258441925\n",
      "2018-01-08 23:08:51.695887 Epoch 6, batch 37, loss 0.22918882966041565\n",
      "Epoch 6, average loss 0.2464027944448832\n",
      "---------------------------------------\n",
      "2018-01-08 23:09:24.933690 Epoch 7, batch 1, loss 0.3016505241394043\n",
      "2018-01-08 23:09:49.606970 Epoch 7, batch 2, loss 0.2406950145959854\n",
      "2018-01-08 23:10:13.461019 Epoch 7, batch 3, loss 0.2755623161792755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08 23:10:38.725381 Epoch 7, batch 4, loss 0.24588987231254578\n",
      "2018-01-08 23:11:03.983525 Epoch 7, batch 5, loss 0.22296534478664398\n",
      "2018-01-08 23:11:29.196842 Epoch 7, batch 6, loss 0.25086480379104614\n",
      "2018-01-08 23:11:55.153094 Epoch 7, batch 7, loss 0.25413912534713745\n",
      "2018-01-08 23:12:19.985018 Epoch 7, batch 8, loss 0.2099301964044571\n",
      "2018-01-08 23:12:44.552336 Epoch 7, batch 9, loss 0.21539244055747986\n",
      "2018-01-08 23:13:09.234241 Epoch 7, batch 10, loss 0.20049864053726196\n",
      "2018-01-08 23:13:33.728617 Epoch 7, batch 11, loss 0.22134268283843994\n",
      "2018-01-08 23:13:58.241681 Epoch 7, batch 12, loss 0.2261444628238678\n",
      "2018-01-08 23:14:22.270178 Epoch 7, batch 13, loss 0.2031102031469345\n",
      "2018-01-08 23:14:46.329690 Epoch 7, batch 14, loss 0.2340647429227829\n",
      "2018-01-08 23:15:10.209492 Epoch 7, batch 15, loss 0.22096680104732513\n",
      "2018-01-08 23:15:34.036824 Epoch 7, batch 16, loss 0.20681211352348328\n",
      "2018-01-08 23:15:57.557718 Epoch 7, batch 17, loss 0.22453221678733826\n",
      "2018-01-08 23:16:21.129465 Epoch 7, batch 18, loss 0.24038352072238922\n",
      "2018-01-08 23:16:44.690827 Epoch 7, batch 19, loss 0.23629051446914673\n",
      "2018-01-08 23:17:08.545254 Epoch 7, batch 20, loss 0.21714699268341064\n",
      "2018-01-08 23:17:32.181973 Epoch 7, batch 21, loss 0.22843368351459503\n",
      "2018-01-08 23:17:55.713589 Epoch 7, batch 22, loss 0.19299305975437164\n",
      "2018-01-08 23:18:19.275719 Epoch 7, batch 23, loss 0.21414470672607422\n",
      "2018-01-08 23:18:42.897827 Epoch 7, batch 24, loss 0.2207958996295929\n",
      "2018-01-08 23:19:06.521475 Epoch 7, batch 25, loss 0.20221349596977234\n",
      "2018-01-08 23:19:30.135321 Epoch 7, batch 26, loss 0.21529272198677063\n",
      "2018-01-08 23:19:53.784851 Epoch 7, batch 27, loss 0.20802366733551025\n",
      "2018-01-08 23:20:17.373590 Epoch 7, batch 28, loss 0.22463835775852203\n",
      "2018-01-08 23:20:40.892503 Epoch 7, batch 29, loss 0.22859878838062286\n",
      "2018-01-08 23:21:04.464989 Epoch 7, batch 30, loss 0.19625312089920044\n",
      "2018-01-08 23:21:27.955278 Epoch 7, batch 31, loss 0.24569426476955414\n",
      "2018-01-08 23:21:51.448199 Epoch 7, batch 32, loss 0.18399623036384583\n",
      "2018-01-08 23:22:14.990358 Epoch 7, batch 33, loss 0.2123669981956482\n",
      "2018-01-08 23:22:38.535519 Epoch 7, batch 34, loss 0.19684961438179016\n",
      "2018-01-08 23:23:02.331306 Epoch 7, batch 35, loss 0.2075527012348175\n",
      "2018-01-08 23:23:26.001069 Epoch 7, batch 36, loss 0.2321292757987976\n",
      "2018-01-08 23:23:29.489128 Epoch 7, batch 37, loss 0.15704041719436646\n",
      "Epoch 7, average loss 0.22203782523000562\n",
      "---------------------------------------\n",
      "2018-01-08 23:24:00.628418 Epoch 8, batch 1, loss 0.2189856767654419\n",
      "2018-01-08 23:24:24.289523 Epoch 8, batch 2, loss 0.2155151069164276\n",
      "2018-01-08 23:24:47.881853 Epoch 8, batch 3, loss 0.2368360459804535\n",
      "2018-01-08 23:25:11.437429 Epoch 8, batch 4, loss 0.22201703488826752\n",
      "2018-01-08 23:25:34.977922 Epoch 8, batch 5, loss 0.21081551909446716\n",
      "2018-01-08 23:25:58.457608 Epoch 8, batch 6, loss 0.23062579333782196\n",
      "2018-01-08 23:26:22.200276 Epoch 8, batch 7, loss 0.18992581963539124\n",
      "2018-01-08 23:26:45.788349 Epoch 8, batch 8, loss 0.20708350837230682\n",
      "2018-01-08 23:27:09.534327 Epoch 8, batch 9, loss 0.18968497216701508\n",
      "2018-01-08 23:27:33.122113 Epoch 8, batch 10, loss 0.21147698163986206\n",
      "2018-01-08 23:27:56.897074 Epoch 8, batch 11, loss 0.23507526516914368\n",
      "2018-01-08 23:28:20.378675 Epoch 8, batch 12, loss 0.24057704210281372\n",
      "2018-01-08 23:28:43.872887 Epoch 8, batch 13, loss 0.2321968376636505\n",
      "2018-01-08 23:29:07.366981 Epoch 8, batch 14, loss 0.2590111196041107\n",
      "2018-01-08 23:29:31.136046 Epoch 8, batch 15, loss 0.21536210179328918\n",
      "2018-01-08 23:29:54.687917 Epoch 8, batch 16, loss 0.21981441974639893\n",
      "2018-01-08 23:30:18.176498 Epoch 8, batch 17, loss 0.18058344721794128\n",
      "2018-01-08 23:30:41.815273 Epoch 8, batch 18, loss 0.22551536560058594\n",
      "2018-01-08 23:31:05.579320 Epoch 8, batch 19, loss 0.19983866810798645\n",
      "2018-01-08 23:31:29.155487 Epoch 8, batch 20, loss 0.18686842918395996\n",
      "2018-01-08 23:31:52.747772 Epoch 8, batch 21, loss 0.2299046814441681\n",
      "2018-01-08 23:32:16.193299 Epoch 8, batch 22, loss 0.19746188819408417\n",
      "2018-01-08 23:32:39.742942 Epoch 8, batch 23, loss 0.21415773034095764\n",
      "2018-01-08 23:33:03.407079 Epoch 8, batch 24, loss 0.22015485167503357\n",
      "2018-01-08 23:33:26.923196 Epoch 8, batch 25, loss 0.24400970339775085\n",
      "2018-01-08 23:33:50.555376 Epoch 8, batch 26, loss 0.2256045937538147\n",
      "2018-01-08 23:34:14.524283 Epoch 8, batch 27, loss 0.23073768615722656\n",
      "2018-01-08 23:34:38.131060 Epoch 8, batch 28, loss 0.1995854377746582\n",
      "2018-01-08 23:35:01.619530 Epoch 8, batch 29, loss 0.2361510545015335\n",
      "2018-01-08 23:35:25.093991 Epoch 8, batch 30, loss 0.19515454769134521\n",
      "2018-01-08 23:35:48.730399 Epoch 8, batch 31, loss 0.2189643830060959\n",
      "2018-01-08 23:36:12.408661 Epoch 8, batch 32, loss 0.24206916987895966\n",
      "2018-01-08 23:36:35.970221 Epoch 8, batch 33, loss 0.20099011063575745\n",
      "2018-01-08 23:36:59.500716 Epoch 8, batch 34, loss 0.18788382411003113\n",
      "2018-01-08 23:37:23.092033 Epoch 8, batch 35, loss 0.2111179232597351\n",
      "2018-01-08 23:37:46.849206 Epoch 8, batch 36, loss 0.2270611822605133\n",
      "2018-01-08 23:37:50.286571 Epoch 8, batch 37, loss 0.3536973297595978\n",
      "Epoch 8, average loss 0.22060852034671888\n",
      "---------------------------------------\n",
      "2018-01-08 23:38:21.674230 Epoch 9, batch 1, loss 0.2286260724067688\n",
      "2018-01-08 23:38:45.225907 Epoch 9, batch 2, loss 0.3670395016670227\n",
      "2018-01-08 23:39:08.860831 Epoch 9, batch 3, loss 0.2659229636192322\n",
      "2018-01-08 23:39:32.539749 Epoch 9, batch 4, loss 0.2808828055858612\n",
      "2018-01-08 23:39:56.246782 Epoch 9, batch 5, loss 0.2862979769706726\n",
      "2018-01-08 23:40:19.876198 Epoch 9, batch 6, loss 0.2679363191127777\n",
      "2018-01-08 23:40:43.446388 Epoch 9, batch 7, loss 0.36688950657844543\n",
      "2018-01-08 23:41:06.949815 Epoch 9, batch 8, loss 0.29653242230415344\n",
      "2018-01-08 23:41:30.561765 Epoch 9, batch 9, loss 0.3157910108566284\n",
      "2018-01-08 23:41:54.109419 Epoch 9, batch 10, loss 0.3295927047729492\n",
      "2018-01-08 23:42:17.595951 Epoch 9, batch 11, loss 0.2850010097026825\n",
      "2018-01-08 23:42:41.046574 Epoch 9, batch 12, loss 0.2790603041648865\n",
      "2018-01-08 23:43:04.770337 Epoch 9, batch 13, loss 0.3098243176937103\n",
      "2018-01-08 23:43:28.354240 Epoch 9, batch 14, loss 0.24858422577381134\n",
      "2018-01-08 23:43:51.887183 Epoch 9, batch 15, loss 0.2872407138347626\n",
      "2018-01-08 23:44:15.406548 Epoch 9, batch 16, loss 0.2662563621997833\n",
      "2018-01-08 23:44:38.906248 Epoch 9, batch 17, loss 0.2801716923713684\n",
      "2018-01-08 23:45:02.446688 Epoch 9, batch 18, loss 0.25012847781181335\n",
      "2018-01-08 23:45:26.026847 Epoch 9, batch 19, loss 0.2104066014289856\n",
      "2018-01-08 23:45:49.665463 Epoch 9, batch 20, loss 0.22947907447814941\n",
      "2018-01-08 23:46:13.193019 Epoch 9, batch 21, loss 0.4279395341873169\n",
      "2018-01-08 23:46:36.924854 Epoch 9, batch 22, loss 0.28175094723701477\n",
      "2018-01-08 23:47:00.613171 Epoch 9, batch 23, loss 0.297119677066803\n",
      "2018-01-08 23:47:24.090173 Epoch 9, batch 24, loss 0.3348836302757263\n",
      "2018-01-08 23:47:47.806922 Epoch 9, batch 25, loss 0.2870316505432129\n",
      "2018-01-08 23:48:11.454607 Epoch 9, batch 26, loss 0.26107990741729736\n",
      "2018-01-08 23:48:35.078893 Epoch 9, batch 27, loss 0.23592446744441986\n",
      "2018-01-08 23:48:58.633424 Epoch 9, batch 28, loss 0.28873419761657715\n",
      "2018-01-08 23:49:22.116284 Epoch 9, batch 29, loss 0.23368258774280548\n",
      "2018-01-08 23:49:45.650115 Epoch 9, batch 30, loss 0.3219614624977112\n",
      "2018-01-08 23:50:09.323712 Epoch 9, batch 31, loss 0.2963763475418091\n",
      "2018-01-08 23:50:33.134106 Epoch 9, batch 32, loss 0.29011040925979614\n",
      "2018-01-08 23:50:56.859888 Epoch 9, batch 33, loss 0.2919028401374817\n",
      "2018-01-08 23:51:20.402568 Epoch 9, batch 34, loss 0.24840682744979858\n",
      "2018-01-08 23:51:43.977043 Epoch 9, batch 35, loss 0.3194149434566498\n",
      "2018-01-08 23:52:07.521772 Epoch 9, batch 36, loss 0.3029535710811615\n",
      "2018-01-08 23:52:11.030766 Epoch 9, batch 37, loss 0.268962025642395\n",
      "Epoch 9, average loss 0.28756484026844437\n",
      "---------------------------------------\n",
      "2018-01-08 23:52:42.545248 Epoch 10, batch 1, loss 0.27418074011802673\n",
      "2018-01-08 23:53:06.242675 Epoch 10, batch 2, loss 0.21005670726299286\n",
      "2018-01-08 23:53:29.811105 Epoch 10, batch 3, loss 0.3180079460144043\n",
      "2018-01-08 23:53:53.615428 Epoch 10, batch 4, loss 0.24635833501815796\n",
      "2018-01-08 23:54:17.291855 Epoch 10, batch 5, loss 0.3208428621292114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08 23:54:40.866286 Epoch 10, batch 6, loss 0.32776233553886414\n",
      "2018-01-08 23:55:04.469045 Epoch 10, batch 7, loss 0.29044729471206665\n",
      "2018-01-08 23:55:28.037185 Epoch 10, batch 8, loss 0.2535068988800049\n",
      "2018-01-08 23:55:51.808294 Epoch 10, batch 9, loss 0.23259443044662476\n",
      "2018-01-08 23:56:15.267371 Epoch 10, batch 10, loss 0.24239715933799744\n",
      "2018-01-08 23:56:38.701734 Epoch 10, batch 11, loss 0.33296075463294983\n",
      "2018-01-08 23:57:02.227576 Epoch 10, batch 12, loss 0.2429637759923935\n",
      "2018-01-08 23:57:26.017764 Epoch 10, batch 13, loss 0.26448339223861694\n",
      "2018-01-08 23:57:49.546352 Epoch 10, batch 14, loss 0.2537609338760376\n",
      "2018-01-08 23:58:13.175317 Epoch 10, batch 15, loss 0.2698958218097687\n",
      "2018-01-08 23:58:36.690722 Epoch 10, batch 16, loss 0.26727354526519775\n",
      "2018-01-08 23:59:00.415037 Epoch 10, batch 17, loss 0.2618742883205414\n",
      "2018-01-08 23:59:24.060518 Epoch 10, batch 18, loss 0.225727379322052\n",
      "2018-01-08 23:59:47.634568 Epoch 10, batch 19, loss 0.2682340145111084\n",
      "2018-01-09 00:00:11.850719 Epoch 10, batch 20, loss 0.24230435490608215\n",
      "2018-01-09 00:00:36.930535 Epoch 10, batch 21, loss 0.22651943564414978\n",
      "2018-01-09 00:01:02.577931 Epoch 10, batch 22, loss 0.2559797465801239\n",
      "2018-01-09 00:01:28.436678 Epoch 10, batch 23, loss 0.24841871857643127\n",
      "2018-01-09 00:01:55.059719 Epoch 10, batch 24, loss 0.2358916997909546\n",
      "2018-01-09 00:02:22.073468 Epoch 10, batch 25, loss 0.2582482397556305\n",
      "2018-01-09 00:02:48.407518 Epoch 10, batch 26, loss 0.24972602725028992\n",
      "2018-01-09 00:03:14.001221 Epoch 10, batch 27, loss 0.229110449552536\n",
      "2018-01-09 00:03:39.463609 Epoch 10, batch 28, loss 0.2548520267009735\n",
      "2018-01-09 00:04:04.762694 Epoch 10, batch 29, loss 0.2053186297416687\n",
      "2018-01-09 00:04:29.181273 Epoch 10, batch 30, loss 0.23322974145412445\n",
      "2018-01-09 00:04:53.659377 Epoch 10, batch 31, loss 0.23811975121498108\n",
      "2018-01-09 00:05:17.792009 Epoch 10, batch 32, loss 0.25045523047447205\n",
      "2018-01-09 00:05:42.392166 Epoch 10, batch 33, loss 0.22214120626449585\n",
      "2018-01-09 00:06:06.979694 Epoch 10, batch 34, loss 0.22805917263031006\n",
      "2018-01-09 00:06:31.514863 Epoch 10, batch 35, loss 0.23829591274261475\n",
      "2018-01-09 00:06:55.950613 Epoch 10, batch 36, loss 0.25630754232406616\n",
      "2018-01-09 00:06:59.474750 Epoch 10, batch 37, loss 0.26542234420776367\n",
      "Epoch 10, average loss 0.2551818606821266\n",
      "---------------------------------------\n",
      "2018-01-09 00:07:32.751184 Epoch 11, batch 1, loss 0.2985689342021942\n",
      "2018-01-09 00:07:57.266537 Epoch 11, batch 2, loss 0.28043490648269653\n",
      "2018-01-09 00:08:21.376522 Epoch 11, batch 3, loss 0.23605263233184814\n",
      "2018-01-09 00:08:45.539299 Epoch 11, batch 4, loss 0.2558535039424896\n",
      "2018-01-09 00:09:09.760725 Epoch 11, batch 5, loss 0.23388352990150452\n",
      "2018-01-09 00:09:34.258075 Epoch 11, batch 6, loss 0.2648540139198303\n",
      "2018-01-09 00:09:58.667551 Epoch 11, batch 7, loss 0.28479230403900146\n",
      "2018-01-09 00:10:22.823721 Epoch 11, batch 8, loss 0.23271523416042328\n",
      "2018-01-09 00:10:47.052831 Epoch 11, batch 9, loss 0.2608940303325653\n",
      "2018-01-09 00:11:11.364027 Epoch 11, batch 10, loss 0.23615868389606476\n",
      "2018-01-09 00:11:35.785499 Epoch 11, batch 11, loss 0.2601248025894165\n",
      "2018-01-09 00:12:00.011102 Epoch 11, batch 12, loss 0.2641443610191345\n",
      "2018-01-09 00:12:24.371157 Epoch 11, batch 13, loss 0.2549496591091156\n",
      "2018-01-09 00:12:48.677815 Epoch 11, batch 14, loss 0.21426206827163696\n",
      "2018-01-09 00:13:13.026350 Epoch 11, batch 15, loss 0.2201973795890808\n",
      "2018-01-09 00:13:37.190250 Epoch 11, batch 16, loss 0.2515764832496643\n",
      "2018-01-09 00:14:01.933868 Epoch 11, batch 17, loss 0.24412746727466583\n",
      "2018-01-09 00:14:26.109217 Epoch 11, batch 18, loss 0.238785058259964\n",
      "2018-01-09 00:14:50.647905 Epoch 11, batch 19, loss 0.2144184112548828\n",
      "2018-01-09 00:15:14.996924 Epoch 11, batch 20, loss 0.23112615942955017\n",
      "2018-01-09 00:15:39.321201 Epoch 11, batch 21, loss 0.20205186307430267\n",
      "2018-01-09 00:16:03.678360 Epoch 11, batch 22, loss 0.2283226102590561\n",
      "2018-01-09 00:16:28.185300 Epoch 11, batch 23, loss 0.2224566638469696\n",
      "2018-01-09 00:16:53.200585 Epoch 11, batch 24, loss 0.2498772144317627\n",
      "2018-01-09 00:17:18.674456 Epoch 11, batch 25, loss 0.2170853316783905\n",
      "2018-01-09 00:17:43.595498 Epoch 11, batch 26, loss 0.22732079029083252\n",
      "2018-01-09 00:18:08.123594 Epoch 11, batch 27, loss 0.2372020184993744\n",
      "2018-01-09 00:18:32.392370 Epoch 11, batch 28, loss 0.24081900715827942\n",
      "2018-01-09 00:18:56.846591 Epoch 11, batch 29, loss 0.22620302438735962\n",
      "2018-01-09 00:19:21.172291 Epoch 11, batch 30, loss 0.25865477323532104\n",
      "2018-01-09 00:19:45.631770 Epoch 11, batch 31, loss 0.20955929160118103\n",
      "2018-01-09 00:20:09.926967 Epoch 11, batch 32, loss 0.20626702904701233\n",
      "2018-01-09 00:20:34.265360 Epoch 11, batch 33, loss 0.20855426788330078\n",
      "2018-01-09 00:20:58.857318 Epoch 11, batch 34, loss 0.25740182399749756\n",
      "2018-01-09 00:21:23.233748 Epoch 11, batch 35, loss 0.2286609411239624\n",
      "2018-01-09 00:21:47.618179 Epoch 11, batch 36, loss 0.24806131422519684\n",
      "2018-01-09 00:21:51.095038 Epoch 11, batch 37, loss 0.18495100736618042\n",
      "Epoch 11, average loss 0.23868563771247864\n",
      "---------------------------------------\n",
      "2018-01-09 00:22:23.727519 Epoch 12, batch 1, loss 0.22199636697769165\n",
      "2018-01-09 00:22:48.177279 Epoch 12, batch 2, loss 0.20814728736877441\n",
      "2018-01-09 00:23:12.485091 Epoch 12, batch 3, loss 0.2154480665922165\n",
      "2018-01-09 00:23:36.849469 Epoch 12, batch 4, loss 0.21686549484729767\n",
      "2018-01-09 00:24:01.092320 Epoch 12, batch 5, loss 0.19966985285282135\n",
      "2018-01-09 00:24:25.284289 Epoch 12, batch 6, loss 0.22590506076812744\n",
      "2018-01-09 00:24:49.623556 Epoch 12, batch 7, loss 0.2191542088985443\n",
      "2018-01-09 00:25:14.044316 Epoch 12, batch 8, loss 0.22562479972839355\n",
      "2018-01-09 00:25:38.467262 Epoch 12, batch 9, loss 0.2229055017232895\n",
      "2018-01-09 00:26:02.913889 Epoch 12, batch 10, loss 0.23343059420585632\n",
      "2018-01-09 00:26:27.116883 Epoch 12, batch 11, loss 0.20863395929336548\n",
      "2018-01-09 00:26:51.559421 Epoch 12, batch 12, loss 0.20143593847751617\n",
      "2018-01-09 00:27:15.767243 Epoch 12, batch 13, loss 0.23618420958518982\n",
      "2018-01-09 00:27:40.101375 Epoch 12, batch 14, loss 0.17162558436393738\n",
      "2018-01-09 00:28:04.519698 Epoch 12, batch 15, loss 0.2206539809703827\n",
      "2018-01-09 00:28:28.924090 Epoch 12, batch 16, loss 0.21043908596038818\n",
      "2018-01-09 00:28:53.039841 Epoch 12, batch 17, loss 0.23535296320915222\n",
      "2018-01-09 00:29:17.323498 Epoch 12, batch 18, loss 0.23425781726837158\n",
      "2018-01-09 00:29:41.749065 Epoch 12, batch 19, loss 0.1940963715314865\n",
      "2018-01-09 00:30:05.994053 Epoch 12, batch 20, loss 0.2508021593093872\n",
      "2018-01-09 00:30:30.458471 Epoch 12, batch 21, loss 0.24146728217601776\n",
      "2018-01-09 00:30:54.531162 Epoch 12, batch 22, loss 0.21389108896255493\n",
      "2018-01-09 00:31:19.243293 Epoch 12, batch 23, loss 0.20544160902500153\n",
      "2018-01-09 00:31:43.717407 Epoch 12, batch 24, loss 0.18505752086639404\n",
      "2018-01-09 00:32:08.144633 Epoch 12, batch 25, loss 0.20983505249023438\n",
      "2018-01-09 00:32:32.332415 Epoch 12, batch 26, loss 0.19323715567588806\n",
      "2018-01-09 00:32:56.413517 Epoch 12, batch 27, loss 0.22251394391059875\n",
      "2018-01-09 00:33:21.440446 Epoch 12, batch 28, loss 0.20678961277008057\n",
      "2018-01-09 00:33:45.706232 Epoch 12, batch 29, loss 0.18765661120414734\n",
      "2018-01-09 00:34:09.806678 Epoch 12, batch 30, loss 0.1652517020702362\n",
      "2018-01-09 00:34:33.878964 Epoch 12, batch 31, loss 0.20417490601539612\n",
      "2018-01-09 00:34:58.268611 Epoch 12, batch 32, loss 0.22306527197360992\n",
      "2018-01-09 00:35:22.609478 Epoch 12, batch 33, loss 0.22551226615905762\n",
      "2018-01-09 00:35:46.987642 Epoch 12, batch 34, loss 0.18112146854400635\n",
      "2018-01-09 00:36:11.475285 Epoch 12, batch 35, loss 0.21668049693107605\n",
      "2018-01-09 00:36:35.754083 Epoch 12, batch 36, loss 0.21595358848571777\n",
      "2018-01-09 00:36:39.310137 Epoch 12, batch 37, loss 0.14543402194976807\n",
      "Epoch 12, average loss 0.2106949433281615\n",
      "---------------------------------------\n",
      "2018-01-09 00:37:12.431570 Epoch 13, batch 1, loss 0.24036437273025513\n",
      "2018-01-09 00:37:36.655594 Epoch 13, batch 2, loss 0.19280029833316803\n",
      "2018-01-09 00:38:01.352351 Epoch 13, batch 3, loss 0.19011391699314117\n",
      "2018-01-09 00:38:25.647197 Epoch 13, batch 4, loss 0.20005714893341064\n",
      "2018-01-09 00:38:49.859736 Epoch 13, batch 5, loss 0.19397076964378357\n",
      "2018-01-09 00:39:14.435564 Epoch 13, batch 6, loss 0.18183636665344238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-09 00:39:38.945430 Epoch 13, batch 7, loss 0.1760045439004898\n",
      "2018-01-09 00:40:03.774747 Epoch 13, batch 8, loss 0.1901189088821411\n",
      "2018-01-09 00:40:28.114350 Epoch 13, batch 9, loss 0.19625595211982727\n",
      "2018-01-09 00:40:52.260610 Epoch 13, batch 10, loss 0.19223091006278992\n",
      "2018-01-09 00:41:16.858372 Epoch 13, batch 11, loss 0.18369540572166443\n",
      "2018-01-09 00:41:41.232192 Epoch 13, batch 12, loss 0.23139598965644836\n",
      "2018-01-09 00:42:05.524725 Epoch 13, batch 13, loss 0.18225736916065216\n",
      "2018-01-09 00:42:29.884163 Epoch 13, batch 14, loss 0.20529824495315552\n",
      "2018-01-09 00:42:54.589667 Epoch 13, batch 15, loss 0.1857418715953827\n",
      "2018-01-09 00:43:19.005887 Epoch 13, batch 16, loss 0.1823497861623764\n",
      "2018-01-09 00:43:43.250346 Epoch 13, batch 17, loss 0.1797570288181305\n",
      "2018-01-09 00:44:07.970170 Epoch 13, batch 18, loss 0.2138483226299286\n",
      "2018-01-09 00:44:32.234680 Epoch 13, batch 19, loss 0.20002198219299316\n",
      "2018-01-09 00:44:56.560720 Epoch 13, batch 20, loss 0.1814439594745636\n",
      "2018-01-09 00:45:21.138383 Epoch 13, batch 21, loss 0.21120211482048035\n",
      "2018-01-09 00:45:45.491908 Epoch 13, batch 22, loss 0.1761888563632965\n",
      "2018-01-09 00:46:09.929584 Epoch 13, batch 23, loss 0.20310111343860626\n",
      "2018-01-09 00:46:34.262435 Epoch 13, batch 24, loss 0.2274225354194641\n",
      "2018-01-09 00:46:58.754157 Epoch 13, batch 25, loss 0.17433390021324158\n",
      "2018-01-09 00:47:23.057527 Epoch 13, batch 26, loss 0.20263102650642395\n",
      "2018-01-09 00:47:47.463274 Epoch 13, batch 27, loss 0.20186641812324524\n",
      "2018-01-09 00:48:11.993366 Epoch 13, batch 28, loss 0.19699978828430176\n",
      "2018-01-09 00:48:36.319337 Epoch 13, batch 29, loss 0.18984946608543396\n",
      "2018-01-09 00:49:00.763169 Epoch 13, batch 30, loss 0.17105552554130554\n",
      "2018-01-09 00:49:24.961928 Epoch 13, batch 31, loss 0.20026421546936035\n",
      "2018-01-09 00:49:49.293949 Epoch 13, batch 32, loss 0.20709934830665588\n",
      "2018-01-09 00:50:13.679018 Epoch 13, batch 33, loss 0.21902348101139069\n",
      "2018-01-09 00:50:38.312673 Epoch 13, batch 34, loss 0.21016015112400055\n",
      "2018-01-09 00:51:02.831518 Epoch 13, batch 35, loss 0.17842286825180054\n",
      "2018-01-09 00:51:27.086073 Epoch 13, batch 36, loss 0.20719897747039795\n",
      "2018-01-09 00:51:30.623118 Epoch 13, batch 37, loss 0.17491033673286438\n",
      "Epoch 13, average loss 0.19598089923729767\n",
      "---------------------------------------\n",
      "2018-01-09 00:52:02.629849 Epoch 14, batch 1, loss 0.21263960003852844\n",
      "2018-01-09 00:52:27.491362 Epoch 14, batch 2, loss 0.17943745851516724\n",
      "2018-01-09 00:52:51.425501 Epoch 14, batch 3, loss 0.1835343837738037\n",
      "2018-01-09 00:53:16.128997 Epoch 14, batch 4, loss 0.16989991068840027\n",
      "2018-01-09 00:53:40.602207 Epoch 14, batch 5, loss 0.2308177500963211\n",
      "2018-01-09 00:54:04.969414 Epoch 14, batch 6, loss 0.18333348631858826\n",
      "2018-01-09 00:54:29.237197 Epoch 14, batch 7, loss 0.17695192992687225\n",
      "2018-01-09 00:54:53.491831 Epoch 14, batch 8, loss 0.19684533774852753\n",
      "2018-01-09 00:55:18.119713 Epoch 14, batch 9, loss 0.19678670167922974\n",
      "2018-01-09 00:55:43.558211 Epoch 14, batch 10, loss 0.19034022092819214\n",
      "2018-01-09 00:56:08.710028 Epoch 14, batch 11, loss 0.15930195152759552\n",
      "2018-01-09 00:56:32.872816 Epoch 14, batch 12, loss 0.18082913756370544\n",
      "2018-01-09 00:56:57.006082 Epoch 14, batch 13, loss 0.19360437989234924\n",
      "2018-01-09 00:57:20.971878 Epoch 14, batch 14, loss 0.18422071635723114\n",
      "2018-01-09 00:57:44.961433 Epoch 14, batch 15, loss 0.18761447072029114\n",
      "2018-01-09 00:58:09.038729 Epoch 14, batch 16, loss 0.1750490963459015\n",
      "2018-01-09 00:58:33.067729 Epoch 14, batch 17, loss 0.19501808285713196\n",
      "2018-01-09 00:58:57.010924 Epoch 14, batch 18, loss 0.16957741975784302\n",
      "2018-01-09 00:59:20.955915 Epoch 14, batch 19, loss 0.17052486538887024\n",
      "2018-01-09 00:59:44.826530 Epoch 14, batch 20, loss 0.16891781985759735\n",
      "2018-01-09 01:00:08.841649 Epoch 14, batch 21, loss 0.17771205306053162\n",
      "2018-01-09 01:00:33.063134 Epoch 14, batch 22, loss 0.22397218644618988\n",
      "2018-01-09 01:00:57.222397 Epoch 14, batch 23, loss 0.20127367973327637\n",
      "2018-01-09 01:01:21.544837 Epoch 14, batch 24, loss 0.17692048847675323\n",
      "2018-01-09 01:01:45.688516 Epoch 14, batch 25, loss 0.18525201082229614\n",
      "2018-01-09 01:02:09.840453 Epoch 14, batch 26, loss 0.16887563467025757\n",
      "2018-01-09 01:02:33.616926 Epoch 14, batch 27, loss 0.18456920981407166\n",
      "2018-01-09 01:02:57.463254 Epoch 14, batch 28, loss 0.21520492434501648\n",
      "2018-01-09 01:03:22.099813 Epoch 14, batch 29, loss 0.196148082613945\n",
      "2018-01-09 01:03:46.894450 Epoch 14, batch 30, loss 0.19649270176887512\n",
      "2018-01-09 01:04:11.949951 Epoch 14, batch 31, loss 0.20527108013629913\n",
      "2018-01-09 01:04:36.769897 Epoch 14, batch 32, loss 0.21124206483364105\n",
      "2018-01-09 01:05:01.399674 Epoch 14, batch 33, loss 0.19246524572372437\n",
      "2018-01-09 01:05:25.538640 Epoch 14, batch 34, loss 0.18740257620811462\n",
      "2018-01-09 01:05:49.971058 Epoch 14, batch 35, loss 0.17163386940956116\n",
      "2018-01-09 01:06:14.371675 Epoch 14, batch 36, loss 0.19060295820236206\n",
      "2018-01-09 01:06:17.926523 Epoch 14, batch 37, loss 0.13841381669044495\n",
      "Epoch 14, average loss 0.1872620892685813\n",
      "---------------------------------------\n",
      "2018-01-09 01:06:49.782588 Epoch 15, batch 1, loss 0.17730306088924408\n",
      "2018-01-09 01:07:13.483024 Epoch 15, batch 2, loss 0.2298448234796524\n",
      "2018-01-09 01:07:37.010866 Epoch 15, batch 3, loss 0.17483045160770416\n",
      "2018-01-09 01:08:00.834047 Epoch 15, batch 4, loss 0.16592943668365479\n",
      "2018-01-09 01:08:24.350167 Epoch 15, batch 5, loss 0.17077121138572693\n",
      "2018-01-09 01:08:47.912822 Epoch 15, batch 6, loss 0.18061751127243042\n",
      "2018-01-09 01:09:11.388048 Epoch 15, batch 7, loss 0.17475980520248413\n",
      "2018-01-09 01:09:34.931101 Epoch 15, batch 8, loss 0.14591768383979797\n",
      "2018-01-09 01:09:58.350574 Epoch 15, batch 9, loss 0.21495632827281952\n",
      "2018-01-09 01:10:21.792846 Epoch 15, batch 10, loss 0.1720358431339264\n",
      "2018-01-09 01:10:45.569755 Epoch 15, batch 11, loss 0.17190057039260864\n",
      "2018-01-09 01:11:09.064694 Epoch 15, batch 12, loss 0.1933424025774002\n",
      "2018-01-09 01:11:32.553069 Epoch 15, batch 13, loss 0.19686415791511536\n",
      "2018-01-09 01:11:56.036198 Epoch 15, batch 14, loss 0.21034225821495056\n",
      "2018-01-09 01:12:19.846982 Epoch 15, batch 15, loss 0.1764761209487915\n",
      "2018-01-09 01:12:43.340896 Epoch 15, batch 16, loss 0.1852734088897705\n",
      "2018-01-09 01:13:07.077139 Epoch 15, batch 17, loss 0.15342003107070923\n",
      "2018-01-09 01:13:30.653072 Epoch 15, batch 18, loss 0.23404917120933533\n",
      "2018-01-09 01:13:54.349659 Epoch 15, batch 19, loss 0.16057121753692627\n",
      "2018-01-09 01:14:17.981752 Epoch 15, batch 20, loss 0.19511014223098755\n",
      "2018-01-09 01:14:41.511606 Epoch 15, batch 21, loss 0.15495076775550842\n",
      "2018-01-09 01:15:05.213716 Epoch 15, batch 22, loss 0.17959380149841309\n",
      "2018-01-09 01:15:28.913654 Epoch 15, batch 23, loss 0.14895281195640564\n",
      "2018-01-09 01:15:52.415493 Epoch 15, batch 24, loss 0.19910219311714172\n",
      "2018-01-09 01:16:15.922495 Epoch 15, batch 25, loss 0.1952989399433136\n",
      "2018-01-09 01:16:39.462894 Epoch 15, batch 26, loss 0.1794709861278534\n",
      "2018-01-09 01:17:03.249135 Epoch 15, batch 27, loss 0.1967301219701767\n",
      "2018-01-09 01:17:26.843471 Epoch 15, batch 28, loss 0.1937669813632965\n",
      "2018-01-09 01:17:50.320114 Epoch 15, batch 29, loss 0.19603489339351654\n",
      "2018-01-09 01:18:13.854477 Epoch 15, batch 30, loss 0.17224723100662231\n",
      "2018-01-09 01:18:37.594569 Epoch 15, batch 31, loss 0.17822837829589844\n",
      "2018-01-09 01:19:01.304600 Epoch 15, batch 32, loss 0.17121779918670654\n",
      "2018-01-09 01:19:24.791379 Epoch 15, batch 33, loss 0.23233315348625183\n",
      "2018-01-09 01:19:48.353711 Epoch 15, batch 34, loss 0.16675665974617004\n",
      "2018-01-09 01:20:12.210331 Epoch 15, batch 35, loss 0.18055540323257446\n",
      "2018-01-09 01:20:35.744990 Epoch 15, batch 36, loss 0.1584881693124771\n",
      "2018-01-09 01:20:39.172855 Epoch 15, batch 37, loss 0.14112092554569244\n",
      "Epoch 15, average loss 0.18186932037005554\n",
      "---------------------------------------\n",
      "2018-01-09 01:21:10.155135 Epoch 16, batch 1, loss 0.17614513635635376\n",
      "2018-01-09 01:21:33.839143 Epoch 16, batch 2, loss 0.19623678922653198\n",
      "2018-01-09 01:21:57.420169 Epoch 16, batch 3, loss 0.1817237287759781\n",
      "2018-01-09 01:22:20.954672 Epoch 16, batch 4, loss 0.18399479985237122\n",
      "2018-01-09 01:22:44.441604 Epoch 16, batch 5, loss 0.15322327613830566\n",
      "2018-01-09 01:23:08.264330 Epoch 16, batch 6, loss 0.1874682903289795\n",
      "2018-01-09 01:23:31.927259 Epoch 16, batch 7, loss 0.1615632027387619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-09 01:23:55.400499 Epoch 16, batch 8, loss 0.15410611033439636\n",
      "2018-01-09 01:24:18.862613 Epoch 16, batch 9, loss 0.1576811671257019\n",
      "2018-01-09 01:24:42.558137 Epoch 16, batch 10, loss 0.1985192596912384\n",
      "2018-01-09 01:25:06.106139 Epoch 16, batch 11, loss 0.17005081474781036\n",
      "2018-01-09 01:25:29.575156 Epoch 16, batch 12, loss 0.1737866848707199\n",
      "2018-01-09 01:25:53.243169 Epoch 16, batch 13, loss 0.18236877024173737\n",
      "2018-01-09 01:26:16.915774 Epoch 16, batch 14, loss 0.16722515225410461\n",
      "2018-01-09 01:26:40.505117 Epoch 16, batch 15, loss 0.19636599719524384\n",
      "2018-01-09 01:27:04.063358 Epoch 16, batch 16, loss 0.13663263618946075\n",
      "2018-01-09 01:27:27.830460 Epoch 16, batch 17, loss 0.1844235360622406\n",
      "2018-01-09 01:27:51.373583 Epoch 16, batch 18, loss 0.20095625519752502\n",
      "2018-01-09 01:28:15.181761 Epoch 16, batch 19, loss 0.1938750147819519\n",
      "2018-01-09 01:28:38.712473 Epoch 16, batch 20, loss 0.19450891017913818\n",
      "2018-01-09 01:29:02.350157 Epoch 16, batch 21, loss 0.1907988041639328\n",
      "2018-01-09 01:29:25.784321 Epoch 16, batch 22, loss 0.18009981513023376\n",
      "2018-01-09 01:29:49.338310 Epoch 16, batch 23, loss 0.1659373939037323\n",
      "2018-01-09 01:30:12.857984 Epoch 16, batch 24, loss 0.19633382558822632\n",
      "2018-01-09 01:30:36.570693 Epoch 16, batch 25, loss 0.194644957780838\n",
      "2018-01-09 01:31:00.047559 Epoch 16, batch 26, loss 0.1934460997581482\n",
      "2018-01-09 01:31:23.508510 Epoch 16, batch 27, loss 0.18510696291923523\n",
      "2018-01-09 01:31:47.010172 Epoch 16, batch 28, loss 0.15880721807479858\n",
      "2018-01-09 01:32:10.555243 Epoch 16, batch 29, loss 0.1660086214542389\n",
      "2018-01-09 01:32:34.249417 Epoch 16, batch 30, loss 0.1706538200378418\n",
      "2018-01-09 01:32:58.051491 Epoch 16, batch 31, loss 0.16207872331142426\n",
      "2018-01-09 01:33:22.215969 Epoch 16, batch 32, loss 0.17745822668075562\n",
      "2018-01-09 01:33:45.821779 Epoch 16, batch 33, loss 0.1463620364665985\n",
      "2018-01-09 01:34:09.420807 Epoch 16, batch 34, loss 0.1910410076379776\n",
      "2018-01-09 01:34:32.956347 Epoch 16, batch 35, loss 0.17885717749595642\n",
      "2018-01-09 01:34:56.456386 Epoch 16, batch 36, loss 0.19466952979564667\n",
      "2018-01-09 01:34:59.889164 Epoch 16, batch 37, loss 0.14530058205127716\n",
      "Epoch 16, average loss 0.17698541444701119\n",
      "---------------------------------------\n",
      "2018-01-09 01:35:31.106891 Epoch 17, batch 1, loss 0.14263519644737244\n",
      "2018-01-09 01:35:54.720748 Epoch 17, batch 2, loss 0.1955949366092682\n",
      "2018-01-09 01:36:18.235867 Epoch 17, batch 3, loss 0.17569540441036224\n",
      "2018-01-09 01:36:41.727936 Epoch 17, batch 4, loss 0.15781530737876892\n",
      "2018-01-09 01:37:05.440697 Epoch 17, batch 5, loss 0.19033724069595337\n",
      "2018-01-09 01:37:29.076634 Epoch 17, batch 6, loss 0.1820555031299591\n",
      "2018-01-09 01:37:52.524854 Epoch 17, batch 7, loss 0.14233577251434326\n",
      "2018-01-09 01:38:16.052055 Epoch 17, batch 8, loss 0.16789758205413818\n",
      "2018-01-09 01:38:39.800156 Epoch 17, batch 9, loss 0.1584237813949585\n",
      "2018-01-09 01:39:03.689129 Epoch 17, batch 10, loss 0.1358104646205902\n",
      "2018-01-09 01:39:27.321393 Epoch 17, batch 11, loss 0.20125702023506165\n",
      "2018-01-09 01:39:50.913993 Epoch 17, batch 12, loss 0.16956950724124908\n",
      "2018-01-09 01:40:14.704937 Epoch 17, batch 13, loss 0.17599673569202423\n",
      "2018-01-09 01:40:38.435624 Epoch 17, batch 14, loss 0.1566869616508484\n",
      "2018-01-09 01:41:01.903488 Epoch 17, batch 15, loss 0.15678828954696655\n",
      "2018-01-09 01:41:25.357876 Epoch 17, batch 16, loss 0.15936897695064545\n",
      "2018-01-09 01:41:49.147635 Epoch 17, batch 17, loss 0.17092576622962952\n",
      "2018-01-09 01:42:12.718287 Epoch 17, batch 18, loss 0.20305319130420685\n",
      "2018-01-09 01:42:36.188445 Epoch 17, batch 19, loss 0.18983034789562225\n",
      "2018-01-09 01:42:59.838915 Epoch 17, batch 20, loss 0.18956279754638672\n",
      "2018-01-09 01:43:23.422251 Epoch 17, batch 21, loss 0.14271707832813263\n",
      "2018-01-09 01:43:46.930433 Epoch 17, batch 22, loss 0.16198386251926422\n",
      "2018-01-09 01:44:10.396923 Epoch 17, batch 23, loss 0.1547616422176361\n",
      "2018-01-09 01:44:33.892415 Epoch 17, batch 24, loss 0.1792183220386505\n",
      "2018-01-09 01:44:57.642063 Epoch 17, batch 25, loss 0.16823634505271912\n",
      "2018-01-09 01:45:21.041271 Epoch 17, batch 26, loss 0.18728870153427124\n",
      "2018-01-09 01:45:44.581513 Epoch 17, batch 27, loss 0.1420866996049881\n",
      "2018-01-09 01:46:08.055681 Epoch 17, batch 28, loss 0.19431935250759125\n",
      "2018-01-09 01:46:31.579027 Epoch 17, batch 29, loss 0.15224602818489075\n",
      "2018-01-09 01:46:55.037276 Epoch 17, batch 30, loss 0.2096584141254425\n",
      "2018-01-09 01:47:18.548639 Epoch 17, batch 31, loss 0.17695024609565735\n",
      "2018-01-09 01:47:42.105911 Epoch 17, batch 32, loss 0.1551463007926941\n",
      "2018-01-09 01:48:05.803540 Epoch 17, batch 33, loss 0.1637040376663208\n",
      "2018-01-09 01:48:29.308402 Epoch 17, batch 34, loss 0.15608267486095428\n",
      "2018-01-09 01:48:52.960256 Epoch 17, batch 35, loss 0.18394708633422852\n",
      "2018-01-09 01:49:16.455927 Epoch 17, batch 36, loss 0.13971321284770966\n",
      "2018-01-09 01:49:19.859015 Epoch 17, batch 37, loss 0.11492766439914703\n",
      "Epoch 17, average loss 0.1676926608826663\n",
      "---------------------------------------\n",
      "2018-01-09 01:49:50.891317 Epoch 18, batch 1, loss 0.18611323833465576\n",
      "2018-01-09 01:50:14.728037 Epoch 18, batch 2, loss 0.18847671151161194\n",
      "2018-01-09 01:50:38.245597 Epoch 18, batch 3, loss 0.15985478460788727\n",
      "2018-01-09 01:51:01.764319 Epoch 18, batch 4, loss 0.1595352590084076\n",
      "2018-01-09 01:51:25.273385 Epoch 18, batch 5, loss 0.1502126306295395\n",
      "2018-01-09 01:51:48.816768 Epoch 18, batch 6, loss 0.181393563747406\n",
      "2018-01-09 01:52:12.347615 Epoch 18, batch 7, loss 0.15431100130081177\n",
      "2018-01-09 01:52:36.204406 Epoch 18, batch 8, loss 0.13510695099830627\n",
      "2018-01-09 01:52:59.785866 Epoch 18, batch 9, loss 0.14122100174427032\n",
      "2018-01-09 01:53:23.405892 Epoch 18, batch 10, loss 0.15576061606407166\n",
      "2018-01-09 01:53:47.088486 Epoch 18, batch 11, loss 0.1491703987121582\n",
      "2018-01-09 01:54:10.951533 Epoch 18, batch 12, loss 0.17356613278388977\n",
      "2018-01-09 01:54:34.425145 Epoch 18, batch 13, loss 0.11601699143648148\n",
      "2018-01-09 01:54:57.972832 Epoch 18, batch 14, loss 0.1317230761051178\n",
      "2018-01-09 01:55:21.438977 Epoch 18, batch 15, loss 0.18958774209022522\n",
      "2018-01-09 01:55:45.258838 Epoch 18, batch 16, loss 0.14085938036441803\n",
      "2018-01-09 01:56:08.790902 Epoch 18, batch 17, loss 0.16749316453933716\n",
      "2018-01-09 01:56:32.297196 Epoch 18, batch 18, loss 0.184198260307312\n",
      "2018-01-09 01:56:55.806015 Epoch 18, batch 19, loss 0.18083326518535614\n",
      "2018-01-09 01:57:19.543804 Epoch 18, batch 20, loss 0.1745004951953888\n",
      "2018-01-09 01:57:43.077521 Epoch 18, batch 21, loss 0.1326608657836914\n",
      "2018-01-09 01:58:06.689201 Epoch 18, batch 22, loss 0.15607863664627075\n",
      "2018-01-09 01:58:30.281306 Epoch 18, batch 23, loss 0.1703140139579773\n",
      "2018-01-09 01:58:53.916620 Epoch 18, batch 24, loss 0.16946662962436676\n",
      "2018-01-09 01:59:17.448627 Epoch 18, batch 25, loss 0.15124942362308502\n",
      "2018-01-09 01:59:40.948614 Epoch 18, batch 26, loss 0.14561666548252106\n",
      "2018-01-09 02:00:04.619435 Epoch 18, batch 27, loss 0.1659146398305893\n",
      "2018-01-09 02:00:29.597961 Epoch 18, batch 28, loss 0.1727660894393921\n",
      "2018-01-09 02:00:54.842177 Epoch 18, batch 29, loss 0.15022462606430054\n",
      "2018-01-09 02:01:20.517231 Epoch 18, batch 30, loss 0.14929570257663727\n",
      "2018-01-09 02:01:46.428739 Epoch 18, batch 31, loss 0.1890658736228943\n",
      "2018-01-09 02:02:12.114733 Epoch 18, batch 32, loss 0.1348382532596588\n",
      "2018-01-09 02:02:37.139361 Epoch 18, batch 33, loss 0.14859119057655334\n",
      "2018-01-09 02:03:02.655625 Epoch 18, batch 34, loss 0.1727137565612793\n",
      "2018-01-09 02:03:28.185546 Epoch 18, batch 35, loss 0.20020067691802979\n",
      "2018-01-09 02:03:54.043097 Epoch 18, batch 36, loss 0.13943368196487427\n",
      "2018-01-09 02:03:57.825542 Epoch 18, batch 37, loss 0.14139464497566223\n",
      "Epoch 18, average loss 0.15972324420471448\n",
      "---------------------------------------\n",
      "2018-01-09 02:04:31.384033 Epoch 19, batch 1, loss 0.15783683955669403\n",
      "2018-01-09 02:04:56.218864 Epoch 19, batch 2, loss 0.1994086503982544\n",
      "2018-01-09 02:05:21.674452 Epoch 19, batch 3, loss 0.18816201388835907\n",
      "2018-01-09 02:05:46.821763 Epoch 19, batch 4, loss 0.15252530574798584\n",
      "2018-01-09 02:06:10.845748 Epoch 19, batch 5, loss 0.16338469088077545\n",
      "2018-01-09 02:06:35.356180 Epoch 19, batch 6, loss 0.1819794774055481\n",
      "2018-01-09 02:06:59.687990 Epoch 19, batch 7, loss 0.16108185052871704\n",
      "2018-01-09 02:07:23.881739 Epoch 19, batch 8, loss 0.1573362648487091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-09 02:07:48.110028 Epoch 19, batch 9, loss 0.15815109014511108\n",
      "2018-01-09 02:08:12.793320 Epoch 19, batch 10, loss 0.13604313135147095\n",
      "2018-01-09 02:08:36.969609 Epoch 19, batch 11, loss 0.12411093711853027\n",
      "2018-01-09 02:09:01.271155 Epoch 19, batch 12, loss 0.17725157737731934\n",
      "2018-01-09 02:09:25.434844 Epoch 19, batch 13, loss 0.17552638053894043\n",
      "2018-01-09 02:09:49.928642 Epoch 19, batch 14, loss 0.1637367308139801\n",
      "2018-01-09 02:10:14.381138 Epoch 19, batch 15, loss 0.15558147430419922\n",
      "2018-01-09 02:10:38.510785 Epoch 19, batch 16, loss 0.15548449754714966\n",
      "2018-01-09 02:11:02.744735 Epoch 19, batch 17, loss 0.1737828254699707\n",
      "2018-01-09 02:11:27.247754 Epoch 19, batch 18, loss 0.1397133320569992\n",
      "2018-01-09 02:11:51.533500 Epoch 19, batch 19, loss 0.1463049352169037\n",
      "2018-01-09 02:12:15.800634 Epoch 19, batch 20, loss 0.1406955122947693\n",
      "2018-01-09 02:12:39.993017 Epoch 19, batch 21, loss 0.13846012949943542\n",
      "2018-01-09 02:13:04.541000 Epoch 19, batch 22, loss 0.16482454538345337\n",
      "2018-01-09 02:13:28.903534 Epoch 19, batch 23, loss 0.12815101444721222\n",
      "2018-01-09 02:13:53.166304 Epoch 19, batch 24, loss 0.1514078676700592\n",
      "2018-01-09 02:14:17.618771 Epoch 19, batch 25, loss 0.14914461970329285\n",
      "2018-01-09 02:14:41.951845 Epoch 19, batch 26, loss 0.15146861970424652\n",
      "2018-01-09 02:15:06.235818 Epoch 19, batch 27, loss 0.13283543288707733\n",
      "2018-01-09 02:15:30.802851 Epoch 19, batch 28, loss 0.15119168162345886\n",
      "2018-01-09 02:15:55.192339 Epoch 19, batch 29, loss 0.14897558093070984\n",
      "2018-01-09 02:16:19.505406 Epoch 19, batch 30, loss 0.13548001646995544\n",
      "2018-01-09 02:16:43.985539 Epoch 19, batch 31, loss 0.18935847282409668\n",
      "2018-01-09 02:17:08.687849 Epoch 19, batch 32, loss 0.16452361643314362\n",
      "2018-01-09 02:17:32.961661 Epoch 19, batch 33, loss 0.15161144733428955\n",
      "2018-01-09 02:17:57.469304 Epoch 19, batch 34, loss 0.12996406853199005\n",
      "2018-01-09 02:18:21.754034 Epoch 19, batch 35, loss 0.13988284766674042\n",
      "2018-01-09 02:18:46.264330 Epoch 19, batch 36, loss 0.13824328780174255\n",
      "2018-01-09 02:18:49.770212 Epoch 19, batch 37, loss 0.11427031457424164\n",
      "Epoch 19, average loss 0.15372678597231168\n",
      "---------------------------------------\n",
      "2018-01-09 02:19:23.725370 Epoch 20, batch 1, loss 0.1666068136692047\n",
      "2018-01-09 02:19:48.364516 Epoch 20, batch 2, loss 0.13551786541938782\n",
      "2018-01-09 02:20:12.509479 Epoch 20, batch 3, loss 0.16000249981880188\n",
      "2018-01-09 02:20:36.573196 Epoch 20, batch 4, loss 0.1501627266407013\n",
      "2018-01-09 02:21:00.798717 Epoch 20, batch 5, loss 0.14005522429943085\n",
      "2018-01-09 02:21:24.844733 Epoch 20, batch 6, loss 0.15773990750312805\n",
      "2018-01-09 02:21:48.834401 Epoch 20, batch 7, loss 0.1557064801454544\n",
      "2018-01-09 02:22:13.051615 Epoch 20, batch 8, loss 0.14965128898620605\n",
      "2018-01-09 02:22:37.293172 Epoch 20, batch 9, loss 0.12859925627708435\n",
      "2018-01-09 02:23:01.560354 Epoch 20, batch 10, loss 0.14723166823387146\n",
      "2018-01-09 02:23:26.084186 Epoch 20, batch 11, loss 0.15856748819351196\n",
      "2018-01-09 02:23:50.277249 Epoch 20, batch 12, loss 0.1584121435880661\n",
      "2018-01-09 02:24:14.353994 Epoch 20, batch 13, loss 0.15642891824245453\n",
      "2018-01-09 02:24:38.549081 Epoch 20, batch 14, loss 0.15631815791130066\n",
      "2018-01-09 02:25:02.920110 Epoch 20, batch 15, loss 0.20039212703704834\n",
      "2018-01-09 02:25:27.202656 Epoch 20, batch 16, loss 0.1305440217256546\n",
      "2018-01-09 02:25:51.228728 Epoch 20, batch 17, loss 0.12614014744758606\n",
      "2018-01-09 02:26:15.451351 Epoch 20, batch 18, loss 0.13859236240386963\n",
      "2018-01-09 02:26:40.061375 Epoch 20, batch 19, loss 0.16044560074806213\n",
      "2018-01-09 02:27:04.450035 Epoch 20, batch 20, loss 0.14156585931777954\n",
      "2018-01-09 02:27:28.486117 Epoch 20, batch 21, loss 0.1304698884487152\n",
      "2018-01-09 02:27:52.934477 Epoch 20, batch 22, loss 0.20157766342163086\n",
      "2018-01-09 02:28:17.481289 Epoch 20, batch 23, loss 0.1719011664390564\n",
      "2018-01-09 02:28:41.738476 Epoch 20, batch 24, loss 0.14732861518859863\n",
      "2018-01-09 02:29:05.946734 Epoch 20, batch 25, loss 0.19933250546455383\n",
      "2018-01-09 02:29:30.146762 Epoch 20, batch 26, loss 0.14957988262176514\n",
      "2018-01-09 02:29:54.614708 Epoch 20, batch 27, loss 0.14494355022907257\n",
      "2018-01-09 02:30:18.790663 Epoch 20, batch 28, loss 0.13685452938079834\n",
      "2018-01-09 02:30:43.151792 Epoch 20, batch 29, loss 0.14066022634506226\n",
      "2018-01-09 02:31:07.369871 Epoch 20, batch 30, loss 0.13867869973182678\n",
      "2018-01-09 02:31:31.828147 Epoch 20, batch 31, loss 0.11732235550880432\n",
      "2018-01-09 02:31:56.023497 Epoch 20, batch 32, loss 0.12585526704788208\n",
      "2018-01-09 02:32:20.307444 Epoch 20, batch 33, loss 0.18171332776546478\n",
      "2018-01-09 02:32:44.486739 Epoch 20, batch 34, loss 0.16086271405220032\n",
      "2018-01-09 02:33:10.177951 Epoch 20, batch 35, loss 0.16786810755729675\n",
      "2018-01-09 02:33:35.567419 Epoch 20, batch 36, loss 0.1655569076538086\n",
      "2018-01-09 02:33:39.086261 Epoch 20, batch 37, loss 0.18685899674892426\n",
      "Epoch 20, average loss 0.1536768908436234\n",
      "---------------------------------------\n",
      "Training Finished. Saving test images to: ./runs/1515494026.84104\n"
     ]
    }
   ],
   "source": [
    "run(\"./models/lr0_001keep0_5ep20loss0_268072330952.ckpt\")\n",
    "#run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revive():\n",
    "    num_classes = 2\n",
    "    image_shape = (160, 576)\n",
    "    epochs = range(3)\n",
    "    batch_size = 64\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    tests.test_for_kitti_dataset(data_dir)\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
    "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
    "    #  https://www.cityscapes-dataset.com/\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Path to vgg model\n",
    "        model_path = os.path.join(\"./models\", 'lr0_001keep0_5ep1loss1_26573096514.ckpt')\n",
    "        # Create function to get batches\n",
    "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
    "\n",
    "        \n",
    "        imported_meta = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "        imported_meta.restore(sess, model_path)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        fcn_cross_entropy_loss_name = 'fcn_cross_entropy_loss:0'\n",
    "        cross_entropy_loss = graph.get_tensor_by_name(fcn_cross_entropy_loss_name)\n",
    "        vgg_input_tensor_name = 'image_input:0'\n",
    "        vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "\n",
    "        input_image = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "        keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "        correct_label = graph.get_tensor_by_name('correct_label:0')\n",
    "        \n",
    "        batch_nr = 0\n",
    "        batch_size = 8\n",
    "        loss_accumul = 0\n",
    "        for image, label in get_batches_fn(batch_size):\n",
    "            loss = sess.run(cross_entropy_loss,\n",
    "                                feed_dict = {input_image: image,\n",
    "                                             correct_label: label,\n",
    "                                             keep_prob: 1})\n",
    "            loss_accumul += loss\n",
    "            print(\"Batch {}, loss {}\".format(batch_nr + 1, loss)) \n",
    "            batch_nr += 1\n",
    "        avg_loss = (loss_accumul / float(batch_nr))\n",
    "        print(\"Average loss  = {}\".format(avg_loss))\n",
    "revive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
